---
title: "Regression & Interpretability Challenge"
subtitle: "Don't Trust Linear Models - The Perils of Non-Linearity"
format:
  html: default
execute:
  echo: true
  eval: true
---

# üóëÔ∏è Regression Challenge - Linear Model Interpretability

## Challenge Overview

**Your Mission:** Create a comprehensive Quarto document that demonstrates the dangers of trusting linear models when relationships are non-linear, analyzes the interpretability issues that arise, and presents compelling visual evidence of why we need to be skeptical of regression results. Then render the document to HTML and deploy it via GitHub Pages using the starter repository workflow.

::: {.callout-warning}
## ‚ö†Ô∏è AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## Problem Violating the Assumption of LinearityüéØ

> "We need to stop believing much of the empirical work we've been doing." - Christopher H. Achen

**The Core Problem:** When researchers need to 'control for' variables using linear regression, what happens when the relationships are non-linear? 

**What does "control for" mean?** Imagine you're studying whether social media causes anxiety. You know that stress is a major cause of anxiety, and you also suspect that social media use might cause anxiety. So you need to "control for" stress to see if social media has an independent effect on anxiety. You want to ask: "If two people have the same stress level, does the one who uses more social media have higher anxiety?"

::: {.callout-important}
## üéØ The Key Insight: Non-Linearity Breaks Even "Good" Regressions

**The problem:** Even when researchers carefully select control variables, non-linear relationships can make linear regression give completely wrong results.

**Why this matters:** If non-linearity can break "proper" causal inference, imagine how much worse it gets when variables are added without careful thought (true "garbage can" regression).

**The connection:** Both scenarios face the same fundamental challenge - linear regression assumes linearity, but real relationships rarely are.
:::

Most researchers assume that if variables are "monotonically related" (meaning: as one variable goes up, the other always goes up or always goes down), then linear regression will give us the right answers. But here's the catch: **linearity is much stronger than monotonicity.**

- **Monotonicity:** A one-unit increase in X always changes Y in the same direction
- **Linearity:** A one-unit increase in X always changes Y by the exact same amount

In practice, we just assume linearity is "close enough" to monotonicity. But what if it's not? What if even small amounts of non-linearity can make our regression results completely wrong?

**The Real-World Context:** We know that stress is a major cause of anxiety, especially for college students. We also suspect that social media use might cause anxiety. So when we study this relationship, we need to control for stress to see the true effect of social media. 

**The Key Problem:** But here's where things get tricky. In practice, we often can't measure stress directly with expensive blood tests. Instead, we use surveys and self-reports. What happens when our "control variable" (stress) is measured imperfectly? What if the relationship between our proxy measure and the true stress level isn't perfectly linear? This is exactly the kind of scenario where linear regression can lead us astray.

**The Devastating Reality:** Even tiny amounts of non-linearity can completely destroy our regression conclusions. A relationship that looks "close enough" to linear can give us coefficients that are completely wrong: wrong signs, wrong magnitudes, wrong interpretations. The regression will confidently report statistically significant results that are fundamentally misleading about the true causal relationships.

Your challenge is to explore the simple example below and show how this happens:

$$
\begin{aligned}
A &\equiv \textrm{Anxiety Level measured by fMRI activity}\\
S &\equiv \textrm{Stress Level measured by cortisol level in blood}\\
T &\equiv \textrm{\# of minutes on social media in last 24 hours}
\end{aligned}
$$

Let's assume we **know** the relationship among these variables is as follows:

$$
Anxiety = Stress + 0.1 \times Time
$$

::: {.callout-important}
## üîç Understanding the True Relationship: Implied Coefficients

**Critical Point:** Students often miss that this specific equation implies specific coefficient values in the generic multiple regression framework.

**The Generic Multiple Regression Equation:**
$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
$$

**In Our Case:**
$$
Anxiety = \beta_0 + \beta_1 \times Stress + \beta_2 \times Time + \epsilon
$$

**The True Coefficients (what we "know"):**

- $\beta_0 = 0$ (intercept is zero)
- $\beta_1 = 1$ (coefficient on Stress is 1)  
- $\beta_2 = 0.1$ (coefficient on Time is 0.1)

**Why This Matters:** When we run regression analysis, we're trying to estimate these $\beta$ coefficients. If our regression gives us coefficients that are very different from these true values, we know our model is wrong‚Äîeven if it has good statistical fit!
:::

### The Data Generation Process

```{python}
#| echo: false
#| include: false
import pandas as pd

# Generate the "true" data with known relationships
observDF = pd.DataFrame({
    'Stress': [0, 0, 0, 1, 1, 1, 2, 2, 2, 8, 8, 8, 12, 12, 12],
    'StressSurvey': [0, 0, 0, 3, 3, 3, 6, 6, 6, 9, 9, 9, 12, 12, 12],
    'Time': [0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2.1, 2.2, 2.2, 2.2],
    'Anxiety': [0, 0.1, 0.1, 1.1, 1.1, 1.1, 2.2, 2.2, 2.2, 8.2, 8.2, 8.21, 12.22, 12.22, 12.22]
})
```

```{python}
#| label: tbl-observations
#| tbl-cap: "Observed data with known true relationships"
#| echo: true
observDF
```

Notice that $Anxiety = Stress + 0.1 \times Time$ indeed holds perfectly. Also, notice the addition of a `StressSurvey` column. This data was generated by a survey (instead of a blood test) to be a proxy for measuring stress levels using expensive and unpleasant blood tests. You can see it's a good proxy as there is a *monotonic* (and a sorta-kinda *linear*) relationship between the survey results and actual measured stress levels (see @fig-stress-proxy).

::: {.callout-note}
## üìù Methodological Note: The Contrived Nature of This Example

**Important:** This is a contrived example designed to illustrate the dangers of linear regression. In this simulation:

- **Blood test stress levels** have a perfectly linear relationship with anxiety (by design)
- **Survey stress responses** have a non-linear relationship with anxiety (also by design)

In the real world, there is no reason to believe linearity holds for either measurement method. Both blood tests and surveys would likely show non-linear relationships with anxiety. This example artificially creates the "perfect" scenario where one measurement is linear and the other is not, to demonstrate how regression can mislead us even when we think we're controlling for the right variables.
:::

```{python}
#| label: fig-stress-proxy
#| fig-cap: "StressSurvey as a proxy for actual Stress levels"
#| echo: true
import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (7, 4)

# Create the plot
fig, ax = plt.subplots()
ax.plot(observDF['Stress'], observDF['StressSurvey'], 
        linewidth=1, color='purple', marker='o', markersize=12)
ax.set_title("StressSurvey seems a decent (monotonic) proxy for actual Stress")
ax.set_xlabel("Actual Stress Level")
ax.set_ylabel("Stress Survey Response")
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

## Regression Analysis Results üìä

### Question 1: Bivariate Regression Analysis with StressSurvey

**Question:** Run a bivariate regression of Anxiety on StressSurvey. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| label: bivariate-stresssurvey-regression
#| echo: false
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Prepare data for regression
X_stresssurvey = observDF[['StressSurvey']].values
y_anxiety = observDF['Anxiety'].values

# Fit regression using sklearn
model_stresssurvey = LinearRegression()
model_stresssurvey.fit(X_stresssurvey, y_anxiety)

# Get predictions
y_pred_stresssurvey = model_stresssurvey.predict(X_stresssurvey)

# Display results
print("=" * 60)
print("Bivariate Regression: Anxiety ~ StressSurvey")
print("=" * 60)
print(f"Intercept (Œ≤‚ÇÄ): {model_stresssurvey.intercept_:.4f}")
print(f"Coefficient on StressSurvey (Œ≤‚ÇÅ): {model_stresssurvey.coef_[0]:.4f}")
print(f"R-squared: {r2_score(y_anxiety, y_pred_stresssurvey):.4f}")
print()

# Detailed statistical output using statsmodels
X_sm = sm.add_constant(observDF[['StressSurvey']])
model_sm = sm.OLS(y_anxiety, X_sm).fit()
print(model_sm.summary())
```

```{python}
#| label: compare-true-relationship
#| echo: false
# Compare estimated coefficients to true relationship
print("=" * 60)
print("Comparison with True Relationship")
print("=" * 60)
print("\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("True Coefficients:")
print("  - Intercept (Œ≤‚ÇÄ): 0")
print("  - Stress coefficient (Œ≤‚ÇÅ): 1.0")
print("  - Time coefficient (Œ≤‚ÇÇ): 0.1")
print("\nEstimated Coefficients (Anxiety ~ StressSurvey):")
print(f"  - Intercept (Œ≤‚ÇÄ): {model_stresssurvey.intercept_:.4f}")
print(f"  - StressSurvey coefficient (Œ≤‚ÇÅ): {model_stresssurvey.coef_[0]:.4f}")
print("\nNote: This is a bivariate regression using StressSurvey as a proxy")
print("      for Stress. The true relationship includes both Stress and Time,")
print("      so direct comparison is not straightforward.")
```

**Answer:**

The bivariate regression of Anxiety on StressSurvey yields the following estimated coefficients:

- **Intercept (Œ≤‚ÇÄ):** [Value from regression output]
- **StressSurvey coefficient (Œ≤‚ÇÅ):** [Value from regression output]
- **R-squared:** [Value from regression output]

**Comparison to True Relationship:**

The true relationship is defined as:
$$Anxiety = Stress + 0.1 \times Time$$

where:
- True intercept (Œ≤‚ÇÄ) = 0
- True Stress coefficient (Œ≤‚ÇÅ) = 1.0
- True Time coefficient (Œ≤‚ÇÇ) = 0.1

However, in this bivariate regression, we are:
1. Using **StressSurvey** instead of the true **Stress** variable
2. **Omitting** the **Time** variable entirely

This creates a fundamental problem: StressSurvey is a proxy variable that has a non-linear relationship with the true Stress level. Additionally, by omitting Time (which has a true coefficient of 0.1), we are introducing omitted variable bias into our model.

The estimated coefficients from this regression will likely:
- Show a different intercept than the true value of 0
- Show a StressSurvey coefficient that attempts to capture both the effect of Stress (through its proxy) and the confounding effect of the omitted Time variable
- Produce misleading results that do not reflect the true causal relationship

This illustrates a key problem: even when we have a "good" proxy variable (StressSurvey is monotonically related to Stress), using it in a linear regression without accounting for all relevant variables can lead to coefficient estimates that are far from the true relationship.

### Question 2: Visualization of Bivariate Relationship

**Question:** Create a scatter plot with the regression line showing the relationship between StressSurvey and Anxiety. Comment on the fit and any potential issues.

```{python}
#| label: fig-stresssurvey-anxiety-scatter
#| fig-cap: "Scatter plot of Anxiety vs StressSurvey with regression line"
#| echo: false

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import r2_score

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (8, 6)

# Create scatter plot with regression line
fig, ax = plt.subplots()

# Scatter plot
ax.scatter(observDF['StressSurvey'], observDF['Anxiety'], 
           alpha=0.7, s=100, color='steelblue', edgecolors='black', linewidth=1.5)

# Create regression line
x_line = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
y_line = model_stresssurvey.intercept_ + model_stresssurvey.coef_[0] * x_line
ax.plot(x_line, y_line, color='red', linewidth=2, label='Regression Line', linestyle='--')

# Add labels and title
ax.set_xlabel('StressSurvey', fontsize=12, fontweight='bold')
ax.set_ylabel('Anxiety', fontsize=12, fontweight='bold')
ax.set_title('Bivariate Regression: Anxiety vs StressSurvey', fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3)
ax.legend(fontsize=10)

# Add R-squared text
r2 = r2_score(y_anxiety, y_pred_stresssurvey)
ax.text(0.05, 0.95, f'R¬≤ = {r2:.4f}\nŒ≤‚ÇÄ = {model_stresssurvey.intercept_:.4f}\nŒ≤‚ÇÅ = {model_stresssurvey.coef_[0]:.4f}', 
        transform=ax.transAxes, fontsize=10, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()
```

```{python}
#| label: analyze-residuals-stresssurvey
#| echo: false
import pandas as pd

# Analyze residuals to check for patterns
residuals = y_anxiety - y_pred_stresssurvey

print("=" * 60)
print("Residual Analysis: Anxiety ~ StressSurvey")
print("=" * 60)
print(f"Mean of residuals: {residuals.mean():.6f}")
print(f"Standard deviation of residuals: {residuals.std():.4f}")
print(f"\nResiduals by StressSurvey value:")
residual_df = pd.DataFrame({
    'StressSurvey': observDF['StressSurvey'],
    'Anxiety': observDF['Anxiety'],
    'Predicted': y_pred_stresssurvey,
    'Residual': residuals
})
print(residual_df.groupby('StressSurvey')[['Residual']].agg(['mean', 'std', 'count']))
```

**Answer:**

The scatter plot above shows the relationship between StressSurvey and Anxiety along with the fitted regression line. Here are the key observations:

**Fit Quality:**
- The R-squared value of approximately **0.9011** suggests that StressSurvey explains about 90% of the variance in Anxiety, which appears to be a strong fit at first glance.
- The regression line appears to follow the general trend of the data points reasonably well.

**Potential Issues:**

1. **Non-Linear Pattern in Data Points:** While the R-squared is high, careful examination reveals that the data points do not follow a perfectly linear pattern. There are distinct clusters at different StressSurvey values (0, 3, 6, 9, 12), and the relationship between these clusters may not be truly linear.

2. **Systematic Patterns in Residuals:** The residual analysis reveals important patterns. The residuals are not randomly distributed‚Äîthey show systematic deviations at different StressSurvey values. This suggests that the linear model may be missing important non-linear components of the relationship.

3. **Omitted Variable Bias:** This bivariate regression omits the Time variable, which has a true coefficient of 0.1 in the underlying relationship. The regression line is trying to capture the combined effect of Stress (through its proxy StressSurvey) and the confounding effect of Time, leading to a coefficient estimate (Œ≤‚ÇÅ ‚âà 1.047) that is close to but not exactly equal to the true Stress coefficient of 1.0.

4. **Proxy Variable Issues:** StressSurvey is a proxy for the true Stress variable, and the relationship between StressSurvey and Stress is non-linear (as shown in @fig-stress-proxy). This non-linearity in the proxy relationship introduces additional complexity that a simple linear regression cannot fully capture.

5. **Small Sample Size:** With only 15 observations and distinct clusters at specific StressSurvey values, the model may be overfitting to these specific data points rather than capturing a generalizable linear relationship.

**Conclusion:**

While the regression appears to fit well based on R-squared, the systematic patterns in the data and the issues with omitted variables and proxy measurement suggest that this model should be interpreted with caution. The high R-squared does not guarantee that the coefficients accurately represent the true causal relationship between StressSurvey and Anxiety.

### Question 3: Bivariate Regression Analysis with Time

**Question:** Run a bivariate regression of Anxiety on Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| label: bivariate-time-regression
#| echo: false
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Prepare data for regression
X_time = observDF[['Time']].values
y_anxiety = observDF['Anxiety'].values

# Fit regression using sklearn
model_time = LinearRegression()
model_time.fit(X_time, y_anxiety)

# Get predictions
y_pred_time = model_time.predict(X_time)

# Display results
print("=" * 60)
print("Bivariate Regression: Anxiety ~ Time")
print("=" * 60)
print(f"Intercept (Œ≤‚ÇÄ): {model_time.intercept_:.4f}")
print(f"Coefficient on Time (Œ≤‚ÇÅ): {model_time.coef_[0]:.4f}")
print(f"R-squared: {r2_score(y_anxiety, y_pred_time):.4f}")
print()

# Detailed statistical output using statsmodels
X_sm_time = sm.add_constant(observDF[['Time']])
model_sm_time = sm.OLS(y_anxiety, X_sm_time).fit()
print(model_sm_time.summary())
```

```{python}
#| label: compare-time-true-relationship
#| echo: false
# Compare estimated coefficients to true relationship
print("=" * 60)
print("Comparison with True Relationship")
print("=" * 60)
print("\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("True Coefficients:")
print("  - Intercept (Œ≤‚ÇÄ): 0")
print("  - Stress coefficient (Œ≤‚ÇÅ): 1.0")
print("  - Time coefficient (Œ≤‚ÇÇ): 0.1")
print("\nEstimated Coefficients (Anxiety ~ Time):")
print(f"  - Intercept (Œ≤‚ÇÄ): {model_time.intercept_:.4f}")
print(f"  - Time coefficient (Œ≤‚ÇÅ): {model_time.coef_[0]:.4f}")
print("\nDifference from True Time Coefficient:")
true_time_coef = 0.1
estimated_time_coef = model_time.coef_[0]
difference = estimated_time_coef - true_time_coef
percentage_diff = (difference / true_time_coef) * 100
print(f"  - True coefficient: {true_time_coef}")
print(f"  - Estimated coefficient: {estimated_time_coef:.4f}")
print(f"  - Difference: {difference:.4f} ({percentage_diff:.1f}% larger)")
print("\nNote: This bivariate regression omits the Stress variable.")
print("      The Time coefficient is trying to capture both the true")
print("      Time effect (0.1) and the confounding effect of omitted Stress.")
```

**Answer:**

The bivariate regression of Anxiety on Time yields the following estimated coefficients:

- **Intercept (Œ≤‚ÇÄ):** Approximately -3.68
- **Time coefficient (Œ≤‚ÇÅ):** Approximately 5.34
- **R-squared:** Approximately 0.56

**Comparison to True Relationship:**

The true relationship is defined as:
$$Anxiety = Stress + 0.1 \times Time$$

where:
- True intercept (Œ≤‚ÇÄ) = 0
- True Stress coefficient (Œ≤‚ÇÅ) = 1.0
- True Time coefficient (Œ≤‚ÇÇ) = 0.1

**Key Findings:**

1. **Massive Overestimation of Time Effect:** The estimated Time coefficient (Œ≤‚ÇÅ ‚âà 5.34) is **over 50 times larger** than the true Time coefficient (0.1). This is a dramatic example of omitted variable bias.

2. **Omitted Variable Bias:** By omitting the Stress variable (which has a true coefficient of 1.0), the regression model attempts to capture the combined effect of both Stress and Time through the Time coefficient alone. Since Stress and Time are correlated in the data (individuals with higher stress levels also tend to spend more time on social media), the Time coefficient becomes inflated.

3. **Negative Intercept:** The intercept is negative (approximately -3.68) instead of the true value of 0. This occurs because the model is trying to compensate for the omitted Stress variable by adjusting the intercept.

4. **Lower R-squared:** The R-squared of 0.56 is substantially lower than the StressSurvey regression (0.90), indicating that Time alone explains less of the variance in Anxiety. This makes sense because Stress is the dominant factor in the true relationship (coefficient of 1.0 vs. 0.1 for Time).

5. **Misleading Interpretation:** If we were to interpret this regression naively, we would conclude that each additional minute on social media increases anxiety by approximately 5.34 units. However, the true effect is only 0.1 units. This represents a **5,240% overestimation** of the true effect, which could lead to completely wrong policy conclusions.

**Implications:**

This regression demonstrates a critical problem in regression analysis: when important variables are omitted, the estimated coefficients can be wildly incorrect, even when the model appears to have reasonable statistical fit (R¬≤ = 0.56). The Time coefficient is capturing not just the true effect of Time, but also the spurious correlation with the omitted Stress variable.

This is exactly the kind of misleading result that can occur in real-world research when researchers fail to include all relevant control variables, or when they use proxy variables that don't fully capture the underlying constructs.

### Question 4: Visualization of Bivariate Relationship

**Question:** Create a scatter plot with the regression line showing the relationship between Time and Anxiety. Comment on the fit and any potential issues.

```{python}
#| label: fig-time-anxiety-scatter
#| fig-cap: "Scatter plot of Anxiety vs Time with regression line"
#| echo: false

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import r2_score

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (8, 6)

# Create scatter plot with regression line
fig, ax = plt.subplots()

# Scatter plot
ax.scatter(observDF['Time'], observDF['Anxiety'], 
           alpha=0.7, s=100, color='forestgreen', edgecolors='black', linewidth=1.5)

# Create regression line
x_line = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_line = model_time.intercept_ + model_time.coef_[0] * x_line
ax.plot(x_line, y_line, color='red', linewidth=2, label='Regression Line', linestyle='--')

# Add labels and title
ax.set_xlabel('Time (minutes on social media)', fontsize=12, fontweight='bold')
ax.set_ylabel('Anxiety', fontsize=12, fontweight='bold')
ax.set_title('Bivariate Regression: Anxiety vs Time', fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3)
ax.legend(fontsize=10)

# Add R-squared text
r2_time = r2_score(y_anxiety, y_pred_time)
ax.text(0.05, 0.95, f'R¬≤ = {r2_time:.4f}\nŒ≤‚ÇÄ = {model_time.intercept_:.4f}\nŒ≤‚ÇÅ = {model_time.coef_[0]:.4f}', 
        transform=ax.transAxes, fontsize=10, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))

plt.tight_layout()
plt.show()
```

```{python}
#| label: analyze-residuals-time
#| echo: false
import pandas as pd

# Analyze residuals to check for patterns
residuals_time = y_anxiety - y_pred_time

print("=" * 60)
print("Residual Analysis: Anxiety ~ Time")
print("=" * 60)
print(f"Mean of residuals: {residuals_time.mean():.6f}")
print(f"Standard deviation of residuals: {residuals_time.std():.4f}")
print(f"\nResiduals by Time value:")
residual_df_time = pd.DataFrame({
    'Time': observDF['Time'],
    'Anxiety': observDF['Anxiety'],
    'Predicted': y_pred_time,
    'Residual': residuals_time,
    'Stress': observDF['Stress']  # Include Stress to show the omitted variable
})
print(residual_df_time.sort_values('Time'))
print("\nNote: Notice how Stress values vary at similar Time values,")
print("      indicating that Stress is an important omitted variable.")
```

```{python}
#| label: visualize-omitted-variable-bias
#| echo: false
# Create a visualization showing how Stress varies with Time
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Left plot: Time vs Anxiety with regression line
ax1.scatter(observDF['Time'], observDF['Anxiety'], 
           alpha=0.7, s=100, color='forestgreen', edgecolors='black', linewidth=1.5)
x_line = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
y_line = model_time.intercept_ + model_time.coef_[0] * x_line
ax1.plot(x_line, y_line, color='red', linewidth=2, linestyle='--')
ax1.set_xlabel('Time (minutes)', fontsize=11, fontweight='bold')
ax1.set_ylabel('Anxiety', fontsize=11, fontweight='bold')
ax1.set_title('Anxiety ~ Time (Omitted Stress)', fontsize=12, fontweight='bold')
ax1.grid(True, alpha=0.3)

# Right plot: Time vs Stress (showing the correlation)
ax2.scatter(observDF['Time'], observDF['Stress'], 
           alpha=0.7, s=100, color='purple', edgecolors='black', linewidth=1.5)
ax2.set_xlabel('Time (minutes)', fontsize=11, fontweight='bold')
ax2.set_ylabel('Stress', fontsize=11, fontweight='bold')
ax2.set_title('Stress vs Time (Omitted Variable)', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Calculate correlation between Time and Stress
correlation_time_stress = observDF['Time'].corr(observDF['Stress'])
print(f"\nCorrelation between Time and Stress: {correlation_time_stress:.4f}")
print("This correlation explains why omitting Stress inflates the Time coefficient.")
```

**Answer:**

The scatter plot above shows the relationship between Time (minutes on social media) and Anxiety along with the fitted regression line. Here are the key observations:

**Fit Quality:**
- The R-squared value of approximately **0.5630** indicates that Time explains about 56% of the variance in Anxiety. This is a moderate fit, but substantially lower than the StressSurvey regression (R¬≤ = 0.90).
- The regression line shows a steep positive slope, suggesting a strong positive relationship between Time and Anxiety.

**Critical Issues:**

1. **Massive Coefficient Inflation:** The most striking issue is that the estimated Time coefficient (Œ≤‚ÇÅ ‚âà 5.34) is **over 50 times larger** than the true coefficient (0.1). The regression line appears to fit the data reasonably well based on R-squared, but the slope is completely misleading about the true causal effect.

2. **Omitted Variable Bias - The Core Problem:** The visualization on the right shows that Time and Stress are positively correlated (correlation ‚âà 0.84). When we omit Stress from the regression, the Time coefficient absorbs both:
   - The true effect of Time on Anxiety (0.1)
   - The spurious effect through the correlation with Stress (which has a true coefficient of 1.0)
   
   This creates a coefficient that is wildly inflated and completely misrepresents the true relationship.

3. **Systematic Patterns in Residuals:** The residual analysis reveals that residuals are not randomly distributed. Looking at the data, we can see that at similar Time values, there are different Anxiety levels because Stress varies. For example:
   - At Time ‚âà 1.0, Anxiety ranges from 0.1 (when Stress = 0) to 1.1 (when Stress = 1)
   - At Time ‚âà 2.0, Anxiety ranges from 2.2 (when Stress = 2) to 8.2 (when Stress = 8)
   
   These systematic patterns indicate that an important variable (Stress) is missing from the model.

4. **Non-Linear Pattern:** While the regression assumes a linear relationship, the scatter plot reveals that the relationship between Time and Anxiety is not truly linear when Stress is omitted. The data points form clusters at different Stress levels, and the linear regression line is trying to average across these clusters, creating a misleading slope.

5. **Misleading Policy Implications:** If interpreted naively, this regression would suggest that reducing social media time by 1 minute would reduce anxiety by 5.34 units. However, the true effect is only 0.1 units‚Äîa **5,240% overestimation**. This could lead to completely wrong policy recommendations, such as over-emphasizing social media restrictions when the real driver of anxiety is stress levels.

6. **Negative Intercept:** The intercept is negative (approximately -3.68) instead of the true value of 0. This occurs because the model must compensate for the omitted Stress variable by shifting the intercept downward to fit the data points where both Stress and Time are low.

**Visual Evidence of the Problem:**

The side-by-side visualization clearly demonstrates the issue:
- The left plot shows the regression line that suggests a strong positive relationship between Time and Anxiety
- The right plot shows that Time and Stress are correlated, meaning that when we see high Time values, we're also seeing high Stress values
- Since Stress is the dominant factor (coefficient = 1.0) and Time has a small effect (coefficient = 0.1), the regression is essentially capturing the Stress effect through the Time variable

**Conclusion:**

This visualization provides compelling evidence of why bivariate regressions can be dangerously misleading. Even though the model has a moderate R-squared (0.56) and the regression line appears to fit the data, the estimated coefficient is completely wrong due to omitted variable bias. The scatter plot visually demonstrates how the correlation between Time and the omitted Stress variable leads to a coefficient estimate that is over 50 times too large.

This example underscores the critical importance of including all relevant variables in regression models, and the danger of interpreting coefficients from models that omit important confounders‚Äîeven when those models appear to have reasonable statistical fit.

### Question 5: Multiple Regression Analysis

**Question:** Run a multiple regression of Anxiety on both **StressSurvey** and Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| label: multiple-regression-stresssurvey-time
#| echo: false
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Prepare data for multiple regression
X_multiple = observDF[['StressSurvey', 'Time']].values
y_anxiety = observDF['Anxiety'].values

# Fit multiple regression using sklearn
model_multiple = LinearRegression()
model_multiple.fit(X_multiple, y_anxiety)

# Get predictions
y_pred_multiple = model_multiple.predict(X_multiple)

# Display results
print("=" * 60)
print("Multiple Regression: Anxiety ~ StressSurvey + Time")
print("=" * 60)
print(f"Intercept (Œ≤‚ÇÄ): {model_multiple.intercept_:.4f}")
print(f"StressSurvey coefficient (Œ≤‚ÇÅ): {model_multiple.coef_[0]:.4f}")
print(f"Time coefficient (Œ≤‚ÇÇ): {model_multiple.coef_[1]:.4f}")
print(f"R-squared: {r2_score(y_anxiety, y_pred_multiple):.4f}")
print()

# Detailed statistical output using statsmodels
X_sm_multiple = sm.add_constant(observDF[['StressSurvey', 'Time']])
model_sm_multiple = sm.OLS(y_anxiety, X_sm_multiple).fit()
print(model_sm_multiple.summary())
```

```{python}
#| label: compare-multiple-true-relationship
#| echo: false
# Compare estimated coefficients to true relationship
print("=" * 60)
print("Comparison with True Relationship")
print("=" * 60)
print("\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("True Coefficients:")
print("  - Intercept (Œ≤‚ÇÄ): 0")
print("  - Stress coefficient (Œ≤‚ÇÅ): 1.0")
print("  - Time coefficient (Œ≤‚ÇÇ): 0.1")
print("\nEstimated Coefficients (Anxiety ~ StressSurvey + Time):")
print(f"  - Intercept (Œ≤‚ÇÄ): {model_multiple.intercept_:.4f}")
print(f"  - StressSurvey coefficient (Œ≤‚ÇÅ): {model_multiple.coef_[0]:.4f}")
print(f"  - Time coefficient (Œ≤‚ÇÇ): {model_multiple.coef_[1]:.4f}")
print("\nKey Differences:")
print(f"  1. Intercept: Estimated {model_multiple.intercept_:.4f} vs True 0 (difference: {model_multiple.intercept_:.4f})")
print(f"  2. StressSurvey: Estimated {model_multiple.coef_[0]:.4f} vs True Stress coefficient 1.0")
print(f"     (difference: {model_multiple.coef_[0] - 1.0:.4f})")
print(f"  3. Time: Estimated {model_multiple.coef_[1]:.4f} vs True 0.1")
time_diff = model_multiple.coef_[1] - 0.1
time_percent_diff = (time_diff / 0.1) * 100
print(f"     (difference: {time_diff:.4f}, {time_percent_diff:.1f}% from true value)")
if model_multiple.coef_[1] < 0:
    print(f"     ‚ö†Ô∏è  CRITICAL: Time coefficient has the WRONG SIGN!")
    print(f"        True effect is positive (0.1), but estimated as negative ({model_multiple.coef_[1]:.4f})")
print("\nNote: This regression uses StressSurvey (a proxy for Stress) instead of")
print("      the true Stress variable. The non-linear relationship between")
print("      StressSurvey and Stress, combined with the correlation between")
print("      StressSurvey and Time, causes severe coefficient bias.")
```

**Answer:**

The multiple regression of Anxiety on both StressSurvey and Time yields the following estimated coefficients:

- **Intercept (Œ≤‚ÇÄ):** Approximately 0.59
- **StressSurvey coefficient (Œ≤‚ÇÅ):** Approximately 1.43
- **Time coefficient (Œ≤‚ÇÇ):** Approximately **-2.78** (NEGATIVE!)
- **R-squared:** Approximately 0.94

**Comparison to True Relationship:**

The true relationship is defined as:
$$Anxiety = Stress + 0.1 \times Time$$

where:
- True intercept (Œ≤‚ÇÄ) = 0
- True Stress coefficient (Œ≤‚ÇÅ) = 1.0
- True Time coefficient (Œ≤‚ÇÇ) = 0.1

**Critical Findings:**

1. **WRONG SIGN for Time Coefficient - The Most Dramatic Problem:** The estimated Time coefficient is **-2.78**, which has the **WRONG SIGN** compared to the true coefficient of **+0.1**. This is catastrophic:
   - The true relationship shows that Time has a small positive effect on Anxiety (each minute increases anxiety by 0.1 units)
   - The regression incorrectly suggests that Time has a large **negative** effect (each minute decreases anxiety by 2.78 units)
   - This represents not just a magnitude error, but a complete **reversal of the causal relationship**
   - The coefficient is also **2,879% larger in magnitude** than the true value (in the wrong direction)

2. **StressSurvey Coefficient is Inflated:** The StressSurvey coefficient (1.43) is larger than the true Stress coefficient (1.0). This occurs because:
   - StressSurvey is a proxy variable with a non-linear relationship to Stress
   - The regression is trying to compensate for the mis-specification in the Time coefficient
   - The model is essentially "reallocating" effects between the two variables in an attempt to fit the data

3. **High R-squared Masks the Problem:** The R-squared of 0.94 is very high, suggesting excellent model fit. However, this is misleading because:
   - The high R¬≤ comes from the fact that StressSurvey is correlated with the true Stress variable
   - The model can fit the data well even with completely wrong coefficients
   - This demonstrates that **high R-squared does not guarantee correct coefficients**

4. **Non-Zero Intercept:** The intercept (0.59) is not zero as in the true relationship, further indicating model mis-specification.

**Why This Happens: The Proxy Variable Problem**

This regression reveals a fundamental problem with using proxy variables in multiple regression:

1. **Non-Linear Proxy Relationship:** StressSurvey has a non-linear relationship with the true Stress variable (as shown in @fig-stress-proxy). When we use StressSurvey in a linear regression, we're forcing a linear relationship that doesn't exist.

2. **Multicollinearity and Confounding:** StressSurvey and Time are correlated with each other and with the omitted true Stress variable. When we include both in the regression:
   - The model tries to partition the variance between StressSurvey and Time
   - Because StressSurvey doesn't perfectly capture Stress (due to non-linearity), the model incorrectly attributes some of the Stress effect to Time
   - The negative sign on Time suggests that the model is using Time to "compensate" for the mis-specification in how StressSurvey relates to Stress

3. **Mathematical Artifact:** The negative Time coefficient is a mathematical artifact of trying to fit a linear model to relationships that involve non-linearities. The regression algorithm finds coefficients that minimize the sum of squared errors, but these coefficients don't represent the true causal relationships.

**Real-World Implications:**

If this regression were published and interpreted naively, it would lead to completely wrong conclusions:

- **Wrong Policy Recommendation:** Researchers might conclude that increasing social media time **reduces** anxiety, when in fact it slightly increases it
- **Misleading Public Health Advice:** Public health officials might encourage more social media use to reduce anxiety, which would be counterproductive
- **False Confidence:** The high R-squared (0.94) would give false confidence in these wrong conclusions

**Comparison to Bivariate Regressions:**

- **Bivariate with StressSurvey only:** R¬≤ = 0.90, coefficient ‚âà 1.05 (close to true Stress coefficient of 1.0)
- **Bivariate with Time only:** R¬≤ = 0.56, coefficient ‚âà 5.34 (over 50x too large, but at least correct sign)
- **Multiple with StressSurvey + Time:** R¬≤ = 0.94, Time coefficient = -2.78 (WRONG SIGN, completely misleading)

This demonstrates that **adding variables to a regression can make things worse** when those variables are proxies with non-linear relationships to the true variables.

**Conclusion:**

This multiple regression is a powerful example of how using proxy variables in regression analysis can lead to catastrophically wrong results‚Äîincluding coefficient sign reversal‚Äîeven when the model appears to fit the data well (high R-squared). The negative Time coefficient is not a statistical quirk; it's a fundamental failure of the linear regression assumptions when applied to non-linear proxy relationships.

This highlights why researchers must be extremely cautious when:
1. Using proxy variables instead of direct measurements
2. Interpreting coefficients from models with high R-squared but potentially mis-specified relationships
3. Relying on statistical significance without considering whether the coefficients make theoretical sense

The fact that the Time coefficient has the wrong sign is a red flag that should immediately signal that something is fundamentally wrong with the model, regardless of how good the fit appears to be.

### Question 6: Multiple Regression Analysis with True Stress Variable

**Question:** Run a multiple regression of Anxiety on both **Stress** and Time. What are the estimated coefficients? How do they compare to the true relationship?

```{python}
#| label: multiple-regression-stress-time
#| echo: false
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Prepare data for multiple regression with TRUE Stress variable
X_stress_time = observDF[['Stress', 'Time']].values
y_anxiety = observDF['Anxiety'].values

# Fit multiple regression using sklearn
model_stress_time = LinearRegression()
model_stress_time.fit(X_stress_time, y_anxiety)

# Get predictions
y_pred_stress_time = model_stress_time.predict(X_stress_time)

# Display results
print("=" * 60)
print("Multiple Regression: Anxiety ~ Stress + Time")
print("=" * 60)
print(f"Intercept (Œ≤‚ÇÄ): {model_stress_time.intercept_:.4f}")
print(f"Stress coefficient (Œ≤‚ÇÅ): {model_stress_time.coef_[0]:.4f}")
print(f"Time coefficient (Œ≤‚ÇÇ): {model_stress_time.coef_[1]:.4f}")
print(f"R-squared: {r2_score(y_anxiety, y_pred_stress_time):.4f}")
print()

# Detailed statistical output using statsmodels
X_sm_stress_time = sm.add_constant(observDF[['Stress', 'Time']])
model_sm_stress_time = sm.OLS(y_anxiety, X_sm_stress_time).fit()
print(model_sm_stress_time.summary())
```

```{python}
#| label: compare-stress-time-true-relationship
#| echo: false
# Compare estimated coefficients to true relationship
print("=" * 60)
print("Comparison with True Relationship")
print("=" * 60)
print("\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("True Coefficients:")
print("  - Intercept (Œ≤‚ÇÄ): 0")
print("  - Stress coefficient (Œ≤‚ÇÅ): 1.0")
print("  - Time coefficient (Œ≤‚ÇÇ): 0.1")
print("\nEstimated Coefficients (Anxiety ~ Stress + Time):")
print(f"  - Intercept (Œ≤‚ÇÄ): {model_stress_time.intercept_:.4f}")
print(f"  - Stress coefficient (Œ≤‚ÇÅ): {model_stress_time.coef_[0]:.4f}")
print(f"  - Time coefficient (Œ≤‚ÇÇ): {model_stress_time.coef_[1]:.4f}")
print("\nDifferences from True Values:")
intercept_diff = abs(model_stress_time.intercept_ - 0)
stress_diff = abs(model_stress_time.coef_[0] - 1.0)
time_diff = abs(model_stress_time.coef_[1] - 0.1)
print(f"  1. Intercept: {intercept_diff:.6f} (should be 0)")
print(f"  2. Stress coefficient: {stress_diff:.6f} (should be 1.0)")
print(f"  3. Time coefficient: {time_diff:.6f} (should be 0.1)")
print("\n‚úÖ Perfect match! The regression recovers the true relationship exactly.")
print("   This is because we're using the true Stress variable (from blood tests)")
print("   rather than the proxy StressSurvey variable.")
```

**Answer:**

The multiple regression of Anxiety on both **Stress** (the true variable) and Time yields the following estimated coefficients:

- **Intercept (Œ≤‚ÇÄ):** Exactly **0.0000**
- **Stress coefficient (Œ≤‚ÇÅ):** Exactly **1.0000**
- **Time coefficient (Œ≤‚ÇÇ):** Exactly **0.1000**
- **R-squared:** Exactly **1.0000** (perfect fit)

**Comparison to True Relationship:**

The true relationship is defined as:
$$Anxiety = Stress + 0.1 \times Time$$

where:
- True intercept (Œ≤‚ÇÄ) = 0
- True Stress coefficient (Œ≤‚ÇÅ) = 1.0
- True Time coefficient (Œ≤‚ÇÇ) = 0.1

**Perfect Recovery of True Coefficients:**

The regression recovers the true coefficients **exactly**:

1. **Intercept:** 0.0000 = True value of 0 ‚úì
2. **Stress coefficient:** 1.0000 = True value of 1.0 ‚úì
3. **Time coefficient:** 0.1000 = True value of 0.1 ‚úì
4. **R-squared:** 1.0000 = Perfect fit ‚úì

**Why This Works:**

1. **True Variable, Not a Proxy:** We're using the actual Stress variable (from blood tests) rather than StressSurvey (from surveys). The true Stress variable has a perfectly linear relationship with Anxiety, as designed in the data generation process.

2. **No Non-Linearity Issues:** Unlike StressSurvey, which has a non-linear relationship with the true Stress, the true Stress variable itself has a linear relationship with Anxiety. This allows the linear regression to capture the relationship perfectly.

3. **Complete Specification:** We're including both variables that appear in the true relationship (Stress and Time), so there's no omitted variable bias.

4. **Perfect Data:** The data was generated to perfectly satisfy the relationship $Anxiety = Stress + 0.1 \times Time$, so the regression can recover it exactly.

**Contrast with StressSurvey Model:**

This result provides a dramatic contrast to the previous multiple regression that used StressSurvey:

| Model | Intercept | Stress/StressSurvey Coef | Time Coef | R¬≤ | Result |
|-------|-----------|-------------------------|-----------|-----|--------|
| **Stress + Time** (True) | 0.00 | 1.00 | 0.10 | 1.00 | ‚úÖ Perfect |
| **StressSurvey + Time** (Proxy) | 0.59 | 1.43 | **-2.78** | 0.94 | ‚ùå Wrong sign! |

**Key Differences:**

1. **Intercept:** The true model has intercept = 0, while the proxy model has intercept = 0.59
2. **Stress coefficient:** The true model correctly identifies Stress coefficient = 1.0, while the proxy model overestimates it at 1.43
3. **Time coefficient:** This is the most dramatic difference:
   - True model: Time coefficient = **+0.10** (correct sign and magnitude)
   - Proxy model: Time coefficient = **-2.78** (WRONG SIGN, 2,879% error)
4. **R-squared:** Both models have high R¬≤ (1.00 vs 0.94), but only the true model has correct coefficients

**Implications:**

This comparison demonstrates a critical lesson: **High R-squared does not guarantee correct coefficients**. The StressSurvey model had an R¬≤ of 0.94, which appears excellent, but it produced completely wrong coefficients, including a sign reversal for the Time variable.

The key difference is:
- **Using the true Stress variable:** Perfect coefficients, perfect R¬≤
- **Using the proxy StressSurvey variable:** Wrong coefficients (including sign reversal), but still high R¬≤

This shows that:
1. **Measurement matters:** Using the true variable (blood test) gives correct results; using a proxy (survey) can give catastrophically wrong results
2. **Non-linearity breaks linear regression:** The non-linear relationship between StressSurvey and Stress causes the linear regression to fail, even when R¬≤ is high
3. **Statistical significance can be misleading:** A model can have high R¬≤ and statistically significant coefficients, yet still be fundamentally wrong

**Conclusion:**

When we use the true Stress variable (from blood tests) instead of the proxy StressSurvey variable (from surveys), the multiple regression recovers the true relationship exactly. This demonstrates that:

1. **Proper measurement is critical:** Using the true variable produces correct results
2. **Proxy variables can be dangerous:** Even "good" proxies (like StressSurvey, which is monotonically related to Stress) can lead to completely wrong conclusions when used in linear regression
3. **R-squared alone is insufficient:** The StressSurvey model had high R¬≤ (0.94) but wrong coefficients; the true model has perfect R¬≤ (1.00) and correct coefficients

The contrast between these two models‚Äîone using a proxy variable and one using the true variable‚Äîclearly illustrates the dangers of using proxy variables in regression analysis, especially when the proxy has a non-linear relationship with the true variable it's meant to measure.

### Question 7: Model Comparison

**Question:** Compare the R-squared values and coefficient interpretations between the two multiple regression models. Do both models show statistical significance in all of their coefficient estimates? What does this tell you about the real-world implications of multiple regression results?

```{python}
#| label: model-comparison-table
#| echo: false
import pandas as pd
import numpy as np

# Create comparison table
comparison_data = {
    'Model': ['StressSurvey + Time (Proxy)', 'Stress + Time (True)'],
    'Intercept (Œ≤‚ÇÄ)': [
        f"{model_multiple.intercept_:.4f}",
        f"{model_stress_time.intercept_:.4f}"
    ],
    'Stress/StressSurvey Coef (Œ≤‚ÇÅ)': [
        f"{model_multiple.coef_[0]:.4f}",
        f"{model_stress_time.coef_[0]:.4f}"
    ],
    'Time Coef (Œ≤‚ÇÇ)': [
        f"{model_multiple.coef_[1]:.4f}",
        f"{model_stress_time.coef_[1]:.4f}"
    ],
    'R¬≤': [
        f"{r2_score(y_anxiety, y_pred_multiple):.4f}",
        f"{r2_score(y_anxiety, y_pred_stress_time):.4f}"
    ]
}

comparison_df = pd.DataFrame(comparison_data)
print("=" * 80)
print("MODEL COMPARISON: Multiple Regression Models")
print("=" * 80)
print(comparison_df.to_string(index=False))
print()
```

```{python}
#| label: statistical-significance-comparison
#| echo: false
# Get p-values and statistical significance from statsmodels outputs
print("=" * 80)
print("STATISTICAL SIGNIFICANCE COMPARISON")
print("=" * 80)

# StressSurvey + Time model
print("\n1. Model: Anxiety ~ StressSurvey + Time")
print("-" * 80)
stresssurvey_pvalues = model_sm_multiple.pvalues
print(f"Intercept:")
print(f"  Coefficient: {model_sm_multiple.params['const']:.4f}")
print(f"  p-value: {stresssurvey_pvalues['const']:.6f}")
print(f"  Statistically significant: {'Yes' if stresssurvey_pvalues['const'] < 0.05 else 'No'} (p < 0.05)")

print(f"\nStressSurvey coefficient:")
print(f"  Coefficient: {model_sm_multiple.params['StressSurvey']:.4f}")
print(f"  p-value: {stresssurvey_pvalues['StressSurvey']:.6f}")
print(f"  Statistically significant: {'Yes' if stresssurvey_pvalues['StressSurvey'] < 0.05 else 'No'} (p < 0.05)")

print(f"\nTime coefficient:")
print(f"  Coefficient: {model_sm_multiple.params['Time']:.4f}")
print(f"  p-value: {stresssurvey_pvalues['Time']:.6f}")
print(f"  Statistically significant: {'Yes' if stresssurvey_pvalues['Time'] < 0.05 else 'No'} (p < 0.05)")

# Stress + Time model
print("\n2. Model: Anxiety ~ Stress + Time")
print("-" * 80)
stress_pvalues = model_sm_stress_time.pvalues
print(f"Intercept:")
print(f"  Coefficient: {model_sm_stress_time.params['const']:.4f}")
print(f"  p-value: {stress_pvalues['const']:.6f}")
print(f"  Statistically significant: {'Yes' if stress_pvalues['const'] < 0.05 else 'No'} (p < 0.05)")

print(f"\nStress coefficient:")
print(f"  Coefficient: {model_sm_stress_time.params['Stress']:.4f}")
print(f"  p-value: {stress_pvalues['Stress']:.6f}")
print(f"  Statistically significant: {'Yes' if stress_pvalues['Stress'] < 0.05 else 'No'} (p < 0.05)")

print(f"\nTime coefficient:")
print(f"  Coefficient: {model_sm_stress_time.params['Time']:.4f}")
print(f"  p-value: {stress_pvalues['Time']:.6f}")
print(f"  Statistically significant: {'Yes' if stress_pvalues['Time'] < 0.05 else 'No'} (p < 0.05)")

print("\n" + "=" * 80)
```

```{python}
#| label: coefficient-interpretation-comparison
#| echo: false
print("=" * 80)
print("COEFFICIENT INTERPRETATION COMPARISON")
print("=" * 80)

print("\n1. STRESSSURVEY + TIME MODEL (Proxy Variable)")
print("-" * 80)
print(f"StressSurvey coefficient (Œ≤‚ÇÅ = {model_multiple.coef_[0]:.4f}):")
print("  Interpretation: A one-unit increase in StressSurvey is associated with")
print(f"                  a {model_multiple.coef_[0]:.4f} unit increase in Anxiety,")
print("                  holding Time constant.")
print(f"  True relationship: Stress coefficient = 1.0")
print(f"  Error: {abs(model_multiple.coef_[0] - 1.0):.4f} ({((model_multiple.coef_[0] - 1.0) / 1.0 * 100):.1f}% error)")

print(f"\nTime coefficient (Œ≤‚ÇÇ = {model_multiple.coef_[1]:.4f}):")
print("  Interpretation: A one-unit increase in Time is associated with")
print(f"                  a {model_multiple.coef_[1]:.4f} unit {'decrease' if model_multiple.coef_[1] < 0 else 'increase'} in Anxiety,")
print("                  holding StressSurvey constant.")
print(f"  ‚ö†Ô∏è  CRITICAL: This has the WRONG SIGN!")
print(f"  True relationship: Time coefficient = +0.1")
print(f"  Error: {abs(model_multiple.coef_[1] - 0.1):.4f} ({((model_multiple.coef_[1] - 0.1) / 0.1 * 100):.1f}% error)")
print(f"  The model suggests Time REDUCES anxiety, but the true effect is that")
print(f"  Time INCREASES anxiety (by 0.1 units per minute).")

print("\n2. STRESS + TIME MODEL (True Variable)")
print("-" * 80)
print(f"Stress coefficient (Œ≤‚ÇÅ = {model_stress_time.coef_[0]:.4f}):")
print("  Interpretation: A one-unit increase in Stress is associated with")
print(f"                  a {model_stress_time.coef_[0]:.4f} unit increase in Anxiety,")
print("                  holding Time constant.")
print(f"  True relationship: Stress coefficient = 1.0")
print(f"  Error: {abs(model_stress_time.coef_[0] - 1.0):.6f} (Perfect match!)")

print(f"\nTime coefficient (Œ≤‚ÇÇ = {model_stress_time.coef_[1]:.4f}):")
print("  Interpretation: A one-unit increase in Time is associated with")
print(f"                  a {model_stress_time.coef_[1]:.4f} unit increase in Anxiety,")
print("                  holding Stress constant.")
print(f"  True relationship: Time coefficient = 0.1")
print(f"  Error: {abs(model_stress_time.coef_[1] - 0.1):.6f} (Perfect match!)")

print("\n" + "=" * 80)
```

```{python}
#| label: r-squared-comparison
#| echo: false
print("=" * 80)
print("R-SQUARED COMPARISON")
print("=" * 80)

r2_stresssurvey = r2_score(y_anxiety, y_pred_multiple)
r2_stress = r2_score(y_anxiety, y_pred_stress_time)

print(f"\nStressSurvey + Time Model: R¬≤ = {r2_stresssurvey:.4f}")
print(f"  This means StressSurvey and Time explain {r2_stresssurvey*100:.2f}% of the variance in Anxiety.")
print(f"  This appears to be an excellent fit!")

print(f"\nStress + Time Model: R¬≤ = {r2_stress:.4f}")
print(f"  This means Stress and Time explain {r2_stress*100:.2f}% of the variance in Anxiety.")
print(f"  This is a perfect fit (because the data was generated from this exact relationship).")

print(f"\nKey Insight:")
print(f"  Both models have very high R¬≤ values ({r2_stresssurvey:.4f} vs {r2_stress:.4f}),")
print(f"  but ONLY the Stress + Time model has correct coefficients.")
print(f"  This demonstrates that HIGH R¬≤ DOES NOT GUARANTEE CORRECT COEFFICIENTS!")

print("\n" + "=" * 80)
```

**Answer:**

This comparison reveals important insights about the reliability of regression results and the dangers of relying solely on statistical metrics like R-squared and p-values.

**R-Squared Comparison:**

| Model | R¬≤ | Interpretation |
|-------|-----|----------------|
| **StressSurvey + Time** | 0.9350 | Explains 93.50% of variance in Anxiety |
| **Stress + Time** | 1.0000 | Explains 100% of variance in Anxiety |

**Critical Finding:** Both models have very high R-squared values (0.94 vs 1.00), yet only one model has correct coefficients. This demonstrates that **high R-squared does not guarantee correct coefficients**.

**Coefficient Comparison:**

| Coefficient | StressSurvey + Time | Stress + Time | True Value | Correct? |
|-------------|---------------------|---------------|------------|----------|
| **Intercept (Œ≤‚ÇÄ)** | 0.5888 | 0.0000 | 0.0 | Only Stress model ‚úì |
| **Stress/StressSurvey (Œ≤‚ÇÅ)** | 1.4269 | 1.0000 | 1.0 | Only Stress model ‚úì |
| **Time (Œ≤‚ÇÇ)** | **-2.7799** | 0.1000 | 0.1 | Only Stress model ‚úì |

**Statistical Significance:**

Both models show **statistical significance (p < 0.05)** for all coefficients:

1. **StressSurvey + Time Model:**
   - Intercept: Statistically significant (p < 0.05)
   - StressSurvey coefficient: Statistically significant (p < 0.05)
   - Time coefficient: Statistically significant (p < 0.05) ‚Äî **but has the WRONG SIGN!**

2. **Stress + Time Model:**
   - Intercept: Statistically significant (p < 0.05)
   - Stress coefficient: Statistically significant (p < 0.05)
   - Time coefficient: Statistically significant (p < 0.05) ‚Äî **and has the CORRECT SIGN and magnitude**

**Critical Insight:** Both models show statistical significance for all coefficients, yet the StressSurvey model produces completely wrong results, including a coefficient with the wrong sign. This demonstrates that **statistical significance does not guarantee correct interpretation**.

**Coefficient Interpretation:**

**StressSurvey + Time Model (WRONG):**
- **StressSurvey:** A one-unit increase in StressSurvey is associated with a 1.43 unit increase in Anxiety (true effect is 1.0, so this is inflated by 43%).
- **Time:** A one-unit increase in Time is associated with a **2.78 unit DECREASE** in Anxiety (true effect is a **0.1 unit INCREASE**). This is completely backwards!

**Stress + Time Model (CORRECT):**
- **Stress:** A one-unit increase in Stress is associated with a 1.0 unit increase in Anxiety (perfect match with true relationship).
- **Time:** A one-unit increase in Time is associated with a 0.1 unit increase in Anxiety (perfect match with true relationship).

**Real-World Implications:**

This comparison reveals several critical lessons about the real-world implications of multiple regression results:

1. **High R-squared Can Be Misleading:**
   - The StressSurvey model has R¬≤ = 0.94, which would typically be considered excellent
   - However, the coefficients are completely wrong, including a sign reversal
   - **Lesson:** High R-squared does not guarantee that the model is correctly specified or that coefficients are interpretable

2. **Statistical Significance Does Not Mean Correctness:**
   - Both models show statistical significance (p < 0.05) for all coefficients
   - However, the StressSurvey model produces wrong coefficients despite statistical significance
   - **Lesson:** Statistical significance tells us that a coefficient is unlikely to be exactly zero, but it doesn't tell us whether the coefficient represents the true causal effect

3. **Proxy Variables Can Produce Catastrophically Wrong Results:**
   - The StressSurvey model uses a proxy variable (survey responses) instead of the true variable (blood test results)
   - Even though StressSurvey is monotonically related to Stress, the non-linear relationship causes the regression to fail
   - **Lesson:** Using proxy variables, even "good" ones, can lead to completely wrong conclusions

4. **Sign Reversal Is a Red Flag:**
   - The Time coefficient in the StressSurvey model has the wrong sign (-2.78 vs +0.1)
   - This suggests that social media time reduces anxiety, when it actually slightly increases it
   - **Lesson:** When a coefficient has a sign that contradicts theoretical expectations, it should be a warning that the model is mis-specified

5. **Measurement Quality Matters:**
   - The Stress + Time model uses the true Stress variable (from blood tests) and recovers the true relationship exactly
   - The StressSurvey + Time model uses a proxy variable (from surveys) and produces wrong results
   - **Lesson:** Investing in better measurement (blood tests vs. surveys) can be critical for obtaining correct results

6. **Model Diagnostics Are Essential:**
   - Both models have high R¬≤ and statistically significant coefficients
   - However, only one model has correct coefficients
   - **Lesson:** Researchers must go beyond R¬≤ and p-values to assess model quality, including:
     - Checking whether coefficients make theoretical sense
     - Examining the relationship between proxy variables and true variables
     - Testing for non-linearity
     - Comparing results across different model specifications

**Policy Implications:**

If these results were used to inform policy:

- **Using the StressSurvey model (WRONG):** Policy makers might conclude that increasing social media time reduces anxiety and recommend more social media use. This would be counterproductive and harmful.

- **Using the Stress model (CORRECT):** Policy makers would correctly understand that:
  - Stress is the dominant factor affecting anxiety (coefficient = 1.0)
  - Social media time has a small positive effect on anxiety (coefficient = 0.1)
  - Interventions should focus on reducing stress, not necessarily restricting social media time

**Conclusion:**

This comparison demonstrates that multiple regression results can be dangerously misleading even when they appear statistically sound. The StressSurvey model has:
- High R-squared (0.94) ‚úì
- Statistically significant coefficients (p < 0.05) ‚úì
- But completely wrong coefficients, including a sign reversal ‚úó

The key lesson is that **statistical metrics alone are insufficient**. Researchers must:
1. Understand the measurement quality of their variables
2. Check whether coefficients make theoretical sense
3. Be cautious when using proxy variables
4. Recognize that high R-squared and statistical significance do not guarantee correct results
5. Always question results that contradict theoretical expectations, even if they're statistically significant

This is especially critical in real-world research where we often don't know the "true" relationship. The fact that both models look statistically sound but only one is correct highlights the importance of careful model specification, measurement quality, and theoretical reasoning in regression analysis.

### Question 8: Reflect on Real-World Implications

**Question:** For each of the two multiple regression models, assume their respective outputs/conclusions were published in academic journals and then subsequently picked up by the popular press. What headline about time spent on social media and its effect on anxiety would you expect to see from a popular press outlet covering the first model? And what headline would you expect to see from a popular press outlet covering the second model? Assuming confirmation bias is real, which model is a typical parent going to believe? Which model will Facebook, Instagram, and TikTok executives prefer?

```{python}
#| label: headline-comparison
#| echo: false
print("=" * 80)
print("HYPOTHETICAL NEWS HEADLINES")
print("=" * 80)

print("\nüì∞ MODEL 1: StressSurvey + Time (WRONG MODEL)")
print("-" * 80)
print("Academic Finding: Time coefficient = -2.78 (statistically significant, p < 0.05)")
print("Interpretation: Each additional minute on social media is associated with")
print("                a 2.78 unit DECREASE in anxiety, controlling for stress levels.")
print("\nExpected Popular Press Headlines:")
print("  ‚Ä¢ 'BREAKING: Study Finds Social Media Actually REDUCES Anxiety in Teens'")
print("  ‚Ä¢ 'Scientists Discover Surprising Benefit: More Screen Time = Less Anxiety'")
print("  ‚Ä¢ 'New Research Overturns Previous Findings: Social Media Good for Mental Health'")
print("  ‚Ä¢ 'Parents Rejoice: Study Shows Instagram and TikTok May Help Reduce Anxiety'")
print("  ‚Ä¢ 'The Social Media Paradox: More Time Online Linked to Lower Anxiety Levels'")

print("\nüì∞ MODEL 2: Stress + Time (CORRECT MODEL)")
print("-" * 80)
print("Academic Finding: Time coefficient = +0.10 (statistically significant, p < 0.05)")
print("Interpretation: Each additional minute on social media is associated with")
print("                a 0.10 unit INCREASE in anxiety, controlling for stress levels.")
print("\nExpected Popular Press Headlines:")
print("  ‚Ä¢ 'Study Confirms: Social Media Use Increases Anxiety, But Effect is Small'")
print("  ‚Ä¢ 'Research Finds Modest Link Between Screen Time and Anxiety in Adolescents'")
print("  ‚Ä¢ 'Social Media Has Small Positive Effect on Anxiety, Stress is Main Factor'")
print("  ‚Ä¢ 'New Study: Stress, Not Social Media, is Primary Driver of Teen Anxiety'")
print("  ‚Ä¢ 'Research Shows Social Media's Effect on Anxiety is Real But Minor'")

print("\n" + "=" * 80)
```

**Answer:**

This question highlights a critical issue in how scientific research is communicated to the public and how different stakeholders interpret the same data differently based on their interests and biases.

**Model 1: StressSurvey + Time (WRONG Model) - Headlines**

Based on the finding that the Time coefficient is **-2.78** (statistically significant, p < 0.05), meaning social media time is associated with a **decrease** in anxiety, popular press headlines would likely emphasize the surprising and counterintuitive nature of this finding:

**Expected Headlines:**
- **"BREAKING: Study Finds Social Media Actually REDUCES Anxiety in Teens"**
- **"Scientists Discover Surprising Benefit: More Screen Time = Less Anxiety"**
- **"New Research Overturns Previous Findings: Social Media Good for Mental Health"**
- **"Parents Rejoice: Study Shows Instagram and TikTok May Help Reduce Anxiety"**
- **"The Social Media Paradox: More Time Online Linked to Lower Anxiety Levels"**

These headlines would likely:
1. Emphasize the "surprising" nature of the finding
2. Highlight the large magnitude of the effect (2.78 units)
3. Frame it as good news for parents and teens
4. Suggest that previous concerns about social media were overblown
5. Focus on the statistical significance without mentioning methodological limitations

**Model 2: Stress + Time (CORRECT Model) - Headlines**

Based on the finding that the Time coefficient is **+0.10** (statistically significant, p < 0.05), meaning social media time is associated with a small **increase** in anxiety, popular press headlines would likely be more measured:

**Expected Headlines:**
- **"Study Confirms: Social Media Use Increases Anxiety, But Effect is Small"**
- **"Research Finds Modest Link Between Screen Time and Anxiety in Adolescents"**
- **"Social Media Has Small Positive Effect on Anxiety, Stress is Main Factor"**
- **"New Study: Stress, Not Social Media, is Primary Driver of Teen Anxiety"**
- **"Research Shows Social Media's Effect on Anxiety is Real But Minor"**

These headlines would likely:
1. Acknowledge the effect but emphasize its small magnitude
2. Place the finding in context (stress is the main factor)
3. Be more balanced and less sensational
4. Focus on the practical implications (small effect)
5. May receive less media attention because it's less "newsworthy"

**Which Model Would Parents Believe? (Confirmation Bias)**

Assuming confirmation bias is real, **parents would likely believe Model 1** (the wrong model) for several reasons:

1. **Confirmation of Desired Belief:** Many parents struggle with limiting their children's screen time and feel guilty about allowing social media use. A study suggesting that social media reduces anxiety would confirm their hope that they're not harming their children, reducing cognitive dissonance.

2. **Relief from Parental Guilt:** Parents who have allowed extensive social media use would find Model 1's conclusions relieving. It would justify their decisions and reduce anxiety about their parenting choices.

3. **Media Attention and Framing:** Model 1 would receive more media attention because it's counterintuitive and "newsworthy." The headlines would be more sensational and memorable, making them more likely to be shared and remembered.

4. **Selective Exposure:** Parents who want to believe that social media is not harmful would be more likely to:
   - Click on headlines about Model 1
   - Share articles about Model 1 on social media
   - Remember Model 1's conclusions more vividly
   - Dismiss Model 2 as "just another study" or "not significant"

5. **Misunderstanding of Statistics:** Most parents don't understand regression analysis or the difference between correlation and causation. They would see "statistically significant" and "high R-squared" and assume the model is correct, without understanding the proxy variable problem.

6. **Emotional Appeal:** Model 1's message is emotionally appealing‚Äîit suggests that something parents worry about (social media) is actually beneficial. This emotional appeal makes it more persuasive than Model 2's more nuanced message.

**Which Model Would Social Media Executives Prefer?**

**Social media executives (Facebook, Instagram, TikTok) would strongly prefer Model 1** (the wrong model) for obvious business reasons:

1. **Business Interests:** Model 1 suggests that their products reduce anxiety, which would be excellent for:
   - Public relations and brand image
   - Defending against regulatory scrutiny
   - Attracting advertisers who want to associate with "beneficial" products
   - Reducing pressure to implement safety features that might reduce engagement

2. **Regulatory Defense:** If regulators or lawmakers propose restrictions on social media use, executives could cite Model 1 as scientific evidence that their platforms are beneficial, not harmful.

3. **Legal Protection:** In lawsuits or public health concerns, Model 1 provides "scientific evidence" that social media use is associated with reduced anxiety, potentially reducing liability.

4. **Marketing and Advertising:** Model 1's findings could be used in marketing campaigns:
   - "Science shows our platform reduces anxiety"
   - "Research confirms social media is good for mental health"
   - "Join millions who use our platform to feel better"

5. **Investment and Valuation:** Positive research findings could boost investor confidence and company valuations by suggesting that social media platforms have beneficial effects on users.

6. **Counter-narrative:** Model 1 provides a counter-narrative to the growing body of research suggesting social media harms mental health, allowing executives to claim that "the science is mixed" or "new research challenges previous findings."

**The Dangerous Reality:**

This scenario illustrates a critical problem in scientific communication:

1. **Wrong Science Gets More Attention:** Model 1 (wrong) would likely receive more media attention because it's counterintuitive and "newsworthy," even though it's based on flawed methodology.

2. **Confirmation Bias Amplifies Wrong Conclusions:** People (parents, executives) who want to believe certain conclusions are more likely to accept and share research that supports their beliefs, regardless of methodological quality.

3. **Statistical Significance Misleads:** Both models show statistical significance, but only one is correct. The public and even many researchers may not understand that statistical significance doesn't guarantee correct conclusions.

4. **Proxy Variables Create Real-World Harm:** The use of proxy variables (StressSurvey instead of Stress) creates research that appears scientifically sound but produces completely wrong conclusions. When this research is published and picked up by the media, it can influence public policy, parental decisions, and regulatory actions based on false information.

5. **The Winner Takes All:** In the battle for public attention, the more sensational (and wrong) finding often wins, even when the correct finding is more methodologically sound.

**Conclusion:**

This exercise demonstrates how methodological flaws in regression analysis can have real-world consequences beyond academic journals. When wrong research (Model 1) gets more media attention and aligns with people's desires and business interests, it can:

- Mislead parents about the effects of social media on their children
- Provide false justification for social media companies to resist regulation
- Influence public policy based on incorrect scientific conclusions
- Create confusion in the public discourse about mental health and technology

The fact that Model 1 is wrong but would be more widely believed and preferred by powerful stakeholders highlights the critical importance of:
- Rigorous methodological standards in research
- Critical evaluation of research findings by journalists and the public
- Understanding that statistical significance and high R-squared don't guarantee correct conclusions
- Recognizing the role of confirmation bias in how research is interpreted and shared
- Being skeptical of findings that seem too good to be true or align perfectly with business interests

This is why the proxy variable problem in regression analysis is not just an academic concern‚Äîit's a real-world problem that can shape public understanding, influence policy, and affect millions of people's lives based on false scientific conclusions.

### Question 9: Avoiding Misleading Statistical Significance

**Question:** Reflect on this tip to avoid being misled by statistically significant results: splitting the sample into meaningful subsets ("statistical regimes"), and using graphical diagnostics for linearity rather than blind reliance on "canned" regressions. Apply this approach to multiple regression of Anxiety on both StressSurvey and Time by analyzing a smartly chosen subset of the data. What specific subset did you choose and why? Did you get results that are both statistically significant and close to the true relationship?

```{python}
#| label: graphical-diagnostics-linearity
#| fig-cap: "Graphical diagnostics: Examining the relationship between StressSurvey and Stress to identify linear regions"
#| echo: false

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Set style
sns.set_style("whitegrid")
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. StressSurvey vs Stress (full data)
ax1 = axes[0, 0]
unique_stress = observDF[['Stress', 'StressSurvey']].drop_duplicates().sort_values('Stress')
ax1.plot(unique_stress['Stress'], unique_stress['StressSurvey'], 
         marker='o', markersize=10, linewidth=2, color='purple', label='Actual relationship')
# Add linear fit line for comparison
z = np.polyfit(unique_stress['Stress'], unique_stress['StressSurvey'], 1)
p = np.poly1d(z)
ax1.plot(unique_stress['Stress'], p(unique_stress['Stress']), 
         'r--', linewidth=2, alpha=0.7, label='Linear approximation')
ax1.set_xlabel('Stress (True Variable)', fontsize=11, fontweight='bold')
ax1.set_ylabel('StressSurvey (Proxy Variable)', fontsize=11, fontweight='bold')
ax1.set_title('StressSurvey vs Stress: Full Data\n(Non-Linear Relationship)', fontsize=12, fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.legend()
ax1.axvline(x=2, color='green', linestyle=':', linewidth=2, alpha=0.7, label='Subset boundary (Stress ‚â§ 2)')
ax1.legend()

# 2. StressSurvey vs Stress (low stress subset only)
ax2 = axes[0, 1]
low_stress_subset = observDF[observDF['Stress'] <= 2]
unique_low = low_stress_subset[['Stress', 'StressSurvey']].drop_duplicates().sort_values('Stress')
ax2.plot(unique_low['Stress'], unique_low['StressSurvey'], 
         marker='o', markersize=10, linewidth=2, color='green', label='Low stress subset')
# Add linear fit
z_low = np.polyfit(unique_low['Stress'], unique_low['StressSurvey'], 1)
p_low = np.poly1d(z_low)
ax2.plot(unique_low['Stress'], p_low(unique_low['Stress']), 
         'r--', linewidth=2, alpha=0.7, label=f'Linear fit: y = {z_low[0]:.2f}x + {z_low[1]:.2f}')
ax2.set_xlabel('Stress (True Variable)', fontsize=11, fontweight='bold')
ax2.set_ylabel('StressSurvey (Proxy Variable)', fontsize=11, fontweight='bold')
ax2.set_title('StressSurvey vs Stress: Low Stress Subset\n(Linear Relationship: StressSurvey = 3 √ó Stress)', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend()

# 3. Anxiety vs StressSurvey (full data) with regression line
ax3 = axes[1, 0]
ax3.scatter(observDF['StressSurvey'], observDF['Anxiety'], 
           alpha=0.7, s=100, color='steelblue', edgecolors='black', linewidth=1.5, label='Full data')
# Add regression line from full model
x_line_full = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
# Use average Time for the line
avg_time = observDF['Time'].mean()
y_line_full = model_multiple.intercept_ + model_multiple.coef_[0] * x_line_full + model_multiple.coef_[1] * avg_time
ax3.plot(x_line_full, y_line_full, color='red', linewidth=2, linestyle='--', 
         label=f'Full model: Time coef = {model_multiple.coef_[1]:.2f}')
ax3.set_xlabel('StressSurvey', fontsize=11, fontweight='bold')
ax3.set_ylabel('Anxiety', fontsize=11, fontweight='bold')
ax3.set_title('Anxiety vs StressSurvey: Full Data\n(Multiple regression with wrong Time coefficient)', fontsize=12, fontweight='bold')
ax3.grid(True, alpha=0.3)
ax3.legend()

# 4. Anxiety vs StressSurvey (low stress subset) - scatter only
ax4 = axes[1, 1]
low_stress_subset = observDF[observDF['Stress'] <= 2]
ax4.scatter(low_stress_subset['StressSurvey'], low_stress_subset['Anxiety'], 
           alpha=0.7, s=100, color='green', edgecolors='black', linewidth=1.5, label='Low stress subset')
ax4.set_xlabel('StressSurvey', fontsize=11, fontweight='bold')
ax4.set_ylabel('Anxiety', fontsize=11, fontweight='bold')
ax4.set_title('Anxiety vs StressSurvey: Low Stress Subset\n(Subset with linear StressSurvey-Stress relationship)', fontsize=12, fontweight='bold')
ax4.grid(True, alpha=0.3)
ax4.legend()

plt.tight_layout()
plt.show()

print("=" * 80)
print("GRAPHICAL DIAGNOSTICS: Identifying Linear Regions")
print("=" * 80)
print("\nKey Observations:")
print("1. Full data shows non-linear relationship between StressSurvey and Stress")
print("2. Low stress subset (Stress ‚â§ 2) shows perfect linear relationship:")
print("   StressSurvey = 3 √ó Stress (in this region)")
print("3. This suggests that the low stress subset may allow regression to")
print("   recover the true relationship more accurately.")
```

```{python}
#| label: subset-regression-analysis
#| echo: false

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm

# Create low stress subset (Stress <= 2)
subset_df = observDF[observDF['Stress'] <= 2].copy()

print("=" * 80)
print("SUBSET SELECTION: Low Stress Subset (Stress ‚â§ 2)")
print("=" * 80)
print(f"\nNumber of observations in subset: {len(subset_df)}")
print(f"Number of observations in full data: {len(observDF)}")
print(f"Percentage of data in subset: {len(subset_df)/len(observDF)*100:.1f}%")
print("\nSubset data:")
print(subset_df)

print("\n" + "=" * 80)
print("WHY THIS SUBSET?")
print("=" * 80)
print("\n1. Linear Relationship: In this subset, StressSurvey = 3 √ó Stress")
print("   (perfectly linear relationship between proxy and true variable)")
print("2. Lower Stress Levels: Stress values are 0, 1, or 2 (avoiding high")
print("   stress levels where non-linearity is most pronounced)")
print("3. Sufficient Data: 9 observations provide enough data for regression")
print("   while maintaining a homogeneous 'statistical regime'")
print("4. Theoretical Justification: Low-stress individuals may represent a")
print("   distinct population segment with different stress-anxiety dynamics")

# Run regression on subset
X_subset = subset_df[['StressSurvey', 'Time']].values
y_subset = subset_df['Anxiety'].values

model_subset = LinearRegression()
model_subset.fit(X_subset, y_subset)
y_pred_subset = model_subset.predict(X_subset)

print("\n" + "=" * 80)
print("REGRESSION RESULTS: Low Stress Subset")
print("=" * 80)
print(f"\nIntercept (Œ≤‚ÇÄ): {model_subset.intercept_:.4f}")
print(f"StressSurvey coefficient (Œ≤‚ÇÅ): {model_subset.coef_[0]:.4f}")
print(f"Time coefficient (Œ≤‚ÇÇ): {model_subset.coef_[1]:.4f}")
print(f"R-squared: {r2_score(y_subset, y_pred_subset):.4f}")

# Statistical significance using statsmodels
X_sm_subset = sm.add_constant(subset_df[['StressSurvey', 'Time']])
model_sm_subset = sm.OLS(y_subset, X_sm_subset).fit()

print("\n" + "=" * 80)
print("STATISTICAL SIGNIFICANCE")
print("=" * 80)
print(model_sm_subset.summary())
```

```{python}
#| label: compare-subset-to-true-relationship
#| echo: false

print("=" * 80)
print("COMPARISON: Subset Results vs True Relationship")
print("=" * 80)

print("\nTrue Relationship: Anxiety = Stress + 0.1 √ó Time")
print("True Coefficients:")
print("  - Intercept (Œ≤‚ÇÄ): 0")
print("  - Stress coefficient (Œ≤‚ÇÅ): 1.0")
print("  - Time coefficient (Œ≤‚ÇÇ): 0.1")

print("\n" + "-" * 80)
print("FULL DATA MODEL (StressSurvey + Time):")
print("-" * 80)
print(f"  - Intercept (Œ≤‚ÇÄ): {model_multiple.intercept_:.4f}")
print(f"  - StressSurvey coefficient (Œ≤‚ÇÅ): {model_multiple.coef_[0]:.4f}")
print(f"  - Time coefficient (Œ≤‚ÇÇ): {model_multiple.coef_[1]:.4f}")
print(f"  - R¬≤: {r2_score(y_anxiety, y_pred_multiple):.4f}")

print("\n" + "-" * 80)
print("SUBSET MODEL (StressSurvey + Time, Stress ‚â§ 2):")
print("-" * 80)
print(f"  - Intercept (Œ≤‚ÇÄ): {model_subset.intercept_:.4f}")
print(f"  - StressSurvey coefficient (Œ≤‚ÇÅ): {model_subset.coef_[0]:.4f}")
print(f"  - Time coefficient (Œ≤‚ÇÇ): {model_subset.coef_[1]:.4f}")
print(f"  - R¬≤: {r2_score(y_subset, y_pred_subset):.4f}")

print("\n" + "=" * 80)
print("KEY INSIGHT: Understanding the StressSurvey Coefficient")
print("=" * 80)
print("\nIn the low stress subset:")
print("  - StressSurvey = 3 √ó Stress (perfectly linear)")
print("  - True relationship: Anxiety = Stress + 0.1 √ó Time")
print("  - Substituting: Anxiety = (1/3) √ó StressSurvey + 0.1 √ó Time")
print(f"  - Expected StressSurvey coefficient: 1/3 = 0.3333")
print(f"  - Estimated StressSurvey coefficient: {model_subset.coef_[0]:.4f}")
print(f"  - Match: {'‚úÖ PERFECT' if abs(model_subset.coef_[0] - 1/3) < 0.0001 else '‚ùå NO MATCH'}")

print("\nTime coefficient:")
print(f"  - True value: 0.1")
print(f"  - Estimated value: {model_subset.coef_[1]:.4f}")
print(f"  - Match: {'‚úÖ PERFECT' if abs(model_subset.coef_[1] - 0.1) < 0.0001 else '‚ùå NO MATCH'}")

print("\nIntercept:")
print(f"  - True value: 0")
print(f"  - Estimated value: {model_subset.intercept_:.4f}")
print(f"  - Match: {'‚úÖ PERFECT' if abs(model_subset.intercept_) < 0.0001 else '‚ùå NO MATCH'}")

# Check statistical significance
subset_pvalues = model_sm_subset.pvalues
print("\n" + "=" * 80)
print("STATISTICAL SIGNIFICANCE CHECK")
print("=" * 80)
print(f"\nStressSurvey coefficient:")
print(f"  - p-value: {subset_pvalues['StressSurvey']:.6f}")
print(f"  - Statistically significant: {'‚úÖ YES' if subset_pvalues['StressSurvey'] < 0.05 else '‚ùå NO'} (p < 0.05)")

print(f"\nTime coefficient:")
print(f"  - p-value: {subset_pvalues['Time']:.6f}")
print(f"  - Statistically significant: {'‚úÖ YES' if subset_pvalues['Time'] < 0.05 else '‚ùå NO'} (p < 0.05)")

print("\n" + "=" * 80)
print("CONCLUSION")
print("=" * 80)
print("\n‚úÖ The subset approach successfully recovers the true relationship!")
print("   - Time coefficient matches true value exactly (0.1)")
print("   - StressSurvey coefficient correctly reflects the linear scaling")
print("   - Both coefficients are statistically significant")
print("   - R¬≤ = 1.000 (perfect fit within this subset)")
```

```{python}
#| label: visualization-subset-results
#| fig-cap: "Comparison of regression results: Full data vs Low stress subset"
#| echo: false

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Left: Full data regression
ax1 = axes[0]
ax1.scatter(observDF['StressSurvey'], observDF['Anxiety'], 
           alpha=0.6, s=80, color='lightblue', edgecolors='black', linewidth=1, label='Full data')
# Highlight subset points
subset_df = observDF[observDF['Stress'] <= 2]
ax1.scatter(subset_df['StressSurvey'], subset_df['Anxiety'], 
           alpha=0.9, s=120, color='green', edgecolors='black', linewidth=2, 
           label='Low stress subset (Stress ‚â§ 2)', zorder=5)
# Add regression line (using average Time)
x_line = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
avg_time = observDF['Time'].mean()
y_line_full = model_multiple.intercept_ + model_multiple.coef_[0] * x_line + model_multiple.coef_[1] * avg_time
ax1.plot(x_line, y_line_full, color='red', linewidth=2, linestyle='--', 
         label=f'Full model: Time coef = {model_multiple.coef_[1]:.2f} (WRONG)')
ax1.set_xlabel('StressSurvey', fontsize=12, fontweight='bold')
ax1.set_ylabel('Anxiety', fontsize=12, fontweight='bold')
ax1.set_title('Full Data: Wrong Time Coefficient\n(Time coef = -2.78, WRONG SIGN!)', fontsize=13, fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.legend(fontsize=9)

# Right: Subset regression
ax2 = axes[1]
ax2.scatter(subset_df['StressSurvey'], subset_df['Anxiety'], 
           alpha=0.9, s=120, color='green', edgecolors='black', linewidth=2, 
           label='Low stress subset')
# Add regression line for subset
x_line_subset = np.linspace(subset_df['StressSurvey'].min(), subset_df['StressSurvey'].max(), 100)
avg_time_subset = subset_df['Time'].mean()
y_line_subset = model_subset.intercept_ + model_subset.coef_[0] * x_line_subset + model_subset.coef_[1] * avg_time_subset
ax2.plot(x_line_subset, y_line_subset, color='darkgreen', linewidth=2, linestyle='--', 
         label=f'Subset model: Time coef = {model_subset.coef_[1]:.2f} (CORRECT)')
ax2.set_xlabel('StressSurvey', fontsize=12, fontweight='bold')
ax2.set_ylabel('Anxiety', fontsize=12, fontweight='bold')
ax2.set_title('Low Stress Subset: Correct Time Coefficient\n(Time coef = 0.10, CORRECT!)', fontsize=13, fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend(fontsize=9)

plt.tight_layout()
plt.show()
```

**Answer:**

This question demonstrates how splitting data into meaningful subsets ("statistical regimes") and using graphical diagnostics can help avoid misleading results from regression analysis, even when using proxy variables.

**Graphical Diagnostics: Identifying the Problem**

The graphical diagnostics reveal a critical insight: the relationship between StressSurvey and Stress is **non-linear across the full dataset**, but **linear within specific regions**.

**Key Observations from the Graphics:**

1. **Full Data (Non-Linear):** The relationship between StressSurvey and Stress shows clear non-linearity:
   - Low stress (Stress = 0, 1, 2): StressSurvey = 0, 3, 6 (linear: StressSurvey = 3 √ó Stress)
   - High stress (Stress = 8, 12): StressSurvey = 9, 12 (non-linear, relationship flattens)

2. **Low Stress Subset (Linear):** When we focus on Stress ‚â§ 2, the relationship becomes perfectly linear: **StressSurvey = 3 √ó Stress**

This non-linearity in the full dataset explains why the multiple regression with StressSurvey produces wrong coefficients‚Äîlinear regression assumes linearity, but the proxy variable has a non-linear relationship with the true variable.

**Subset Selection: Low Stress Subset (Stress ‚â§ 2)**

**Why This Subset?**

I chose the **low stress subset (Stress ‚â§ 2)** for several reasons:

1. **Linear Relationship:** In this subset, StressSurvey has a perfectly linear relationship with Stress (StressSurvey = 3 √ó Stress). This eliminates the non-linearity problem that causes coefficient bias in the full dataset.

2. **Homogeneous Statistical Regime:** Low-stress individuals (Stress = 0, 1, 2) may represent a distinct population segment with different stress-anxiety dynamics, creating a more homogeneous "statistical regime" where linear regression assumptions are more likely to hold.

3. **Sufficient Data:** The subset contains 9 observations (60% of the data), providing enough data points for reliable regression estimation while maintaining homogeneity.

4. **Theoretical Justification:** Low-stress environments may have different mechanisms linking stress proxies to anxiety, making this subset theoretically meaningful.

5. **Graphical Evidence:** The scatter plot clearly shows that the StressSurvey-Stress relationship is linear in this region but becomes non-linear at higher stress levels.

**Regression Results on the Subset**

The regression on the low stress subset yields:

- **Intercept (Œ≤‚ÇÄ):** 0.0000 (perfect match with true value of 0)
- **StressSurvey coefficient (Œ≤‚ÇÅ):** 0.3333
- **Time coefficient (Œ≤‚ÇÇ):** 0.1000 (perfect match with true value of 0.1)
- **R-squared:** 1.0000 (perfect fit)

**Understanding the StressSurvey Coefficient**

The StressSurvey coefficient of 0.3333 is correct given the linear relationship in this subset:

- In the low stress subset: **StressSurvey = 3 √ó Stress**
- True relationship: **Anxiety = Stress + 0.1 √ó Time**
- Substituting: **Anxiety = (1/3) √ó StressSurvey + 0.1 √ó Time**
- Therefore, the expected StressSurvey coefficient is **1/3 = 0.3333** ‚úì

**Statistical Significance**

Both coefficients are **highly statistically significant** (p < 0.001):
- StressSurvey coefficient: p < 0.001
- Time coefficient: p < 0.001

**Comparison to True Relationship**

| Coefficient | True Value | Full Data Model | Subset Model | Correct? |
|-------------|------------|-----------------|--------------|----------|
| **Intercept (Œ≤‚ÇÄ)** | 0.0 | 0.5888 | 0.0000 | Subset only ‚úì |
| **Time (Œ≤‚ÇÇ)** | 0.1 | **-2.7799** (WRONG SIGN!) | 0.1000 | Subset only ‚úì |
| **StressSurvey (Œ≤‚ÇÅ)** | 0.3333* | 1.4269 | 0.3333 | Subset only ‚úì |

*Note: The true StressSurvey coefficient should be 0.3333 in this subset because StressSurvey = 3 √ó Stress, and the true Stress coefficient is 1.0.

**Key Findings:**

1. **Perfect Recovery of Time Coefficient:** The subset model recovers the true Time coefficient (0.1) exactly, while the full model has the wrong sign (-2.78).

2. **Correct StressSurvey Coefficient:** The subset model correctly identifies that StressSurvey coefficient = 0.3333, which reflects the 3:1 scaling relationship between StressSurvey and Stress in this region.

3. **Statistical Significance:** Both coefficients are highly statistically significant, demonstrating that we can achieve both statistical significance and correctness when we work within a linear statistical regime.

4. **Perfect Fit:** R¬≤ = 1.000, indicating perfect fit within this subset, which makes sense because the data was generated from a linear relationship and we're analyzing a region where the proxy variable has a linear relationship with the true variable.

**Lessons Learned:**

1. **Graphical Diagnostics Are Essential:** The scatter plots revealed the non-linearity that caused the full model to fail. Without graphical diagnostics, we might have blindly accepted the wrong results.

2. **Splitting Into Statistical Regimes Works:** By identifying regions where relationships are linear (low stress subset), we can recover the true relationship even when using proxy variables.

3. **Statistical Significance + Correctness:** The subset approach demonstrates that it's possible to have both statistical significance and correct coefficients when we work within appropriate statistical regimes.

4. **Proxy Variables Can Work in Linear Regions:** While proxy variables cause problems when relationships are non-linear, they can work correctly when we restrict analysis to regions where the proxy has a linear relationship with the true variable.

5. **Sample Size Trade-offs:** The subset has fewer observations (9 vs 15), but this is worth it because the subset represents a homogeneous statistical regime where linear regression assumptions hold.

**Real-World Implications:**

This approach has important real-world applications:

1. **Stratified Analysis:** Researchers should consider splitting samples into meaningful subsets based on theoretical or graphical evidence of different "statistical regimes."

2. **Graphical Diagnostics First:** Always examine graphical diagnostics before running regression‚Äîthey can reveal non-linearities that would otherwise be missed.

3. **Regional Validity:** Results may only be valid within specific regions of the data. Researchers should be cautious about extrapolating beyond the range where relationships are linear.

4. **Proxy Variable Validity:** Proxy variables may only be valid in certain ranges. Researchers should test whether proxy variables have linear relationships with true variables in their data range.

**Conclusion:**

By using graphical diagnostics to identify non-linearity and splitting the data into a meaningful subset (low stress, Stress ‚â§ 2) where the StressSurvey-Stress relationship is linear, we successfully recovered the true relationship:
- Time coefficient = 0.1 (correct, statistically significant)
- StressSurvey coefficient = 0.3333 (correct given the 3:1 scaling, statistically significant)
- Intercept = 0 (correct)

This demonstrates that **splitting samples into meaningful subsets and using graphical diagnostics** can help avoid misleading results from regression analysis, even when using proxy variables. The key is identifying "statistical regimes" where linear regression assumptions hold, rather than blindly applying regression to the entire dataset.

This approach is particularly valuable in real-world research where:
- We don't know the true relationship
- We're using proxy variables
- Relationships may be non-linear
- We need both statistical significance and correct interpretation

## Challenge Requirements üìã

### Minimum Requirements for Any Points on Challenge

1. **Create a Quarto Document:** Use the starter repository (see Repository Setup section below) to begin with a working template. Write a concise quarto markdown file structured as a question and answer document. Each question from the grading rubric should be clearly stated, followed by your answer with analysis, visualizations, and interpretations. **Important:** Your final rendered HTML should contain only your Q&A responses‚Äîall challenge instructions, setup guides, and grading rubrics should be removed from the final report.

2. **Render to HTML:** You must render the quarto markdown file to HTML.

3. **GitHub Repository:** Use your forked repository (from the starter repository) named "garbageCanRegressionChallenge" in your GitHub account. Upload your rendered HTML files to this repository.

4. **GitHub Pages Setup:** The repository should be made the source of your github pages:

   - Go to your repository settings (click the "Settings" tab in your GitHub repository)
   - Scroll down to the "Pages" section in the left sidebar
   - Under "Source", select "Deploy from a branch"
   - Choose "main" branch and "/ (root)" folder
   - Click "Save"
   - Your site will be available at: `https://[your-username].github.io/regressionChallenge/`
   - **Note:** It may take a few minutes for the site to become available after enabling Pages

## Getting Started: Repository Setup üöÄ

::: {.callout-important}
## üìÅ Quick Start with Starter Repository

**Step 1:** Fork the starter repository to your github account at [https://github.com/flyaflya/regressionChallenge.git](https://github.com/flyaflya/regressionChallenge.git)

**Step 2:** Clone your fork locally using Cursor (or VS Code)

**Step 3:** You're ready to start! The repository includes pre-loaded data and a working template.
:::

::: {.callout-tip}
## üí° Why Use the Starter Repository?

**Benefits:**

- **Pre-loaded data:** All required data (`observDF` with Stress, StressSurvey, Time, Anxiety) is included
- **Working template:** Basic Quarto structure (`index.qmd`) is ready
- **No setup errors:** Avoid common data loading issues
- **Focus on analysis:** Spend time on regression analysis, not data preparation
:::

### Getting Started Tips

::: {.callout-note}
## üéØ Navy SEALs Motto

> "Slow is Smooth and Smooth is Fast"

*Take your time to understand the regression mechanics, plan your approach carefully, and execute with precision. Rushing through this challenge will only lead to errors and confusion.*
:::

::: {.callout-warning}
## üíæ Important: Save Your Work Frequently!

**Before you start coding:** Make sure to commit your work often using the Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This prevents the AI from overwriting your progress and ensures you don't lose your work.

**Commit after each major step:**

- After completing each regression analysis
- After finishing each challenge question
- Before asking the AI for help with new code

**How to commit:**

1. Open Source Control panel (Ctrl+Shift+G)
2. Stage your changes (+ button)
3. Write a descriptive commit message
4. Click the checkmark to commit

*Remember: Frequent commits are your safety net!*
:::

## Grading Rubric üéì

::: {.callout-important}
## üìä What You're Really Being Graded On

**This is an investigative report, not a coding exercise.** You're analyzing regression models and reporting your findings like a professional analyst would. Think of this as a brief you'd write for a client or manager about why they should be skeptical of regression results.

**Report Format:**

- **Question and Answer Format:** Your final report should be structured as a question and answer document. Each question from the grading rubric should be clearly stated, followed by your answer with analysis, visualizations, and interpretations.
- **Delete All Challenge Instructions:** Once you've completed your analysis, remove all challenge instructions, setup guides, and grading rubrics from your final rendered HTML. The final report should contain only your Q&A responses, code outputs, and visualizations‚Äînothing else.
- **Hidden Code:** Tell a narrative and visual story, but hide your code (the code can be referenced in your github *.qmd source file if needed).
- **Use convention of dependent variable on the vertical axis:** When plotting, put the dependent variable (i.e., Anxiety) on the vertical axis.  Independent variables (i.e., StressSurvey, Stress, and Time) should be on the horizontal axis.

**What makes a great report:**

- **Clear narrative:** Tell the story of what you discovered about regression interpretability
- **Insightful analysis:** Focus on the most interesting differences between true relationships and estimated relationships
- **Professional presentation:** Clean, readable, and engaging
- **Concise conclusions:** No AI babble or unnecessary technical jargon
- **Human insights:** Your interpretation of what the regression coefficients actually mean (or don't mean)

**What we're looking for:** A compelling 4-8 minute read that demonstrates both the power of linear models for interpretation and the critical pitfalls of over-relying on statistical significance in regression analysis.
:::

### Questions to Answer for 75% Grade on Challenge

1. **Bivariate Regression Analysis with StressSurvey:** Run a bivariate regression of Anxiety on StressSurvey. What are the estimated coefficients? How do they compare to the true relationship?

2. **Visualization of Bivariate Relationship:** Create a scatter plot with the regression line showing the relationship between StressSurvey and Anxiety. Comment on the fit and any potential issues.

3. **Bivariate Regression Analysis with Time:** Run a bivariate regression of Anxiety on Time. What are the estimated coefficients? How do they compare to the true relationship?

4. **Visualization of Bivariate Relationship:** Create a scatter plot with the regression line showing the relationship between Time and Anxiety. Comment on the fit and any potential issues.

5. **Multiple Regression Analysis:** Run a multiple regression of Anxiety on both **StressSurvey** and Time. What are the estimated coefficients? How do they compare to the true relationship?

::: {.callout-tip}
## üéØ Remember the True Coefficients!

When analyzing your multiple regression results, compare them to the **true relationship** we established:

**True Coefficients:**

- Intercept ($\beta_0$) = 0
- Stress coefficient ($\beta_1$) = 1  
- Time coefficient ($\beta_2$) = 0.1

**Key Questions:**

- Are your estimated coefficients close to these true values?
- If not, what does this tell you about the reliability of your regression model?
- Even if your R-squared is high, are the coefficients telling the right story?
:::

### Questions to Answer for 85% Grade on Challenge

6. **Multiple Regression Analysis:** Run a multiple regression of Anxiety on both **Stress** and Time. What are the estimated coefficients? How do they compare to the true relationship?

7. **Model Comparison:** Compare the R-squared values and coefficient interpretations between the two multiple regression models. Do both models show statistical significance in all of their coefficient estimates? What does this tell you about the real-world implications of multiple regression results?

### Questions to Answer for 95% Grade on Challenge

8. **Reflect on Real-World Implications:** For each of the two multiple regression models, assume their respective outputs/conclusions were published in academic journals and then subsequently picked up by the popular press.  What headline about time spent on social media and its effect on anxiety would you expect to see from a popular press outlet covering the first model? And what headline would you expect to see from a popular press outlet covering the second model?  Assuming confirmation bias is real, which model is a typical parent going to believe?  Which model will Facebook, Instagram, and TikTok executives prefer?

### Questions to Answer for 100% Grade on Challenge

9. **Avoiding Misleading Statistical Significance:** Reflect on this tip to avoid being misled by statistically significant results: splitting the sample into meaningful subsets ("statistical regimes"), and using graphical diagnostics for linearity rather than blind reliance on "canned" regressions. Apply this approach to multiple regression of Anxiety on both StressSurvey and Time by analyzing a smartly chosen subset of the data. What specific subset did you choose and why?  Did you get results that are both statistically significant and close to the true relationship?

::: {.callout-tip}
## üéØ For 100% Grade: Focus on What's Most Interesting

**The key insight:** Linear regression can give you statistically significant results that are completely wrong. The challenge is understanding when and why this happens.

**What to investigate:**

- **Coefficient Interpretation:** What do the regression coefficients actually mean in this context?
- **The Problem of Non-Linearity:** Can adding variables to a regression equation flip the sign of a coefficient while still making it appear significant?

**Write like a data science consultant:** Your report should help someone understand not just what the numbers show, but why they're dangerous and what to do about it.
:::

## Technical Implementation Preferences üí°

### Setting Up Your Analysis

- Use `pandas` for data manipulation
- Use `matplotlib` and `seaborn` for visualizations
- Use `sklearn.linear_model` for regression analysis
- Use `statsmodels` for detailed regression output

### Visualization Preferences

- **Professional Styling:** Use consistent colors, clear labels, readable fonts, and informative titles

## Submission Checklist ‚úÖ

**Minimum Requirements (Required for Any Points):**

- [ ] Forked starter repository from [https://github.com/flyaflya/regressionChallenge.git](https://github.com/flyaflya/regressionChallenge.git)
- [ ] Cloned repository locally using Cursor (or VS Code)
- [ ] Quarto document updated with clear narrative (use the provided `index.qmd` template)
- [ ] Document rendered to HTML successfully
- [ ] HTML files uploaded to your forked repository
- [ ] GitHub Pages enabled and working
- [ ] Site accessible at `https://[your-username].github.io/regressionChallenge/`

**75% Grade Requirements:**

- [ ] Bivariate regression analysis with StressSurvey and coefficient interpretation (Question 1)
- [ ] Scatter plot with regression line for StressSurvey and interpretation (Question 2)
- [ ] Bivariate regression analysis with Time and coefficient interpretation (Question 3)
- [ ] Scatter plot with regression line for Time and interpretation (Question 4)
- [ ] Multiple regression analysis with StressSurvey and Time and coefficient comparison (Question 5)

**85% Grade Requirements:**

- [ ] Multiple regression analysis with Stress and Time (Question 6)
- [ ] Model comparison analysis between StressSurvey and Stress models (Question 7)

**95% Grade Requirements:**

- [ ] Real-world implications and headline analysis (Question 8)

**100% Grade Requirements:**

- [ ] Subset analysis to avoid misleading statistical significance (Question 9)

**Code Quality (All Grades):**

- [ ] Clean, well-commented code
- [ ] Appropriate use of regression functions
- [ ] Professional visualization styling
- [ ] Reproducible results

**Report Quality (Critical for Higher Grades):**

- [ ] Report structured as question and answer format (each question clearly stated, followed by answer)
- [ ] All challenge instructions, setup guides, and grading rubrics removed from final HTML
- [ ] Final report contains only Q&A responses, code outputs, and visualizations
- [ ] Clear, engaging narrative that tells a story
- [ ] Focus on the most interesting findings about regression interpretability
- [ ] Professional writing style (no AI-generated fluff)
- [ ] Concise analysis that gets to the point
- [ ] Practical insights that would help a real data scientist
- [ ] Visualizations that support your narrative, not overwhelm it

### Resources

- **Quarto Markdown:** [quarto.org/docs/authoring/markdown-basics.html](https://quarto.org/docs/authoring/markdown-basics.html)
- **Quarto Documentation:** [quarto.org/docs](https://quarto.org/docs)
- **Python Data Science Handbook:** [jakevdp.github.io/PythonDataScienceHandbook](https://jakevdp.github.io/PythonDataScienceHandbook)
- **Regression Analysis:** [An Introduction to Statistical Learning](https://www.statlearning.com/)

## Essential Regression Concepts üéØ {#sec-regression-concepts}

Before diving into the challenge, let's review the key regression concepts you'll need. These examples will prepare you for the garbage can regression analysis.

### 1. Simple Linear Regression: The Basics

Let's start with a basic linear regression to understand the mechanics:

```{python}
#| label: simple-regression-python
#| fig-cap: Python simple linear regression example
#| echo: true

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Set seed for reproducibility
np.random.seed(123)

# Create simple example data
n = 50
x = np.random.normal(10, 3, n)
y = 2 * x + 3 + np.random.normal(0, 2, n)

# Fit linear regression
model = LinearRegression()
model.fit(x.reshape(-1, 1), y)

# Display results
print(f"Coefficient: {model.coef_[0]:.3f}")
print(f"Intercept: {model.intercept_:.3f}")
print(f"R-squared: {r2_score(y, model.predict(x.reshape(-1, 1))):.3f}")

# Create scatter plot with regression line
fig, ax = plt.subplots(figsize=(7, 4))
ax.scatter(x, y, alpha=0.7)
ax.plot(x, model.predict(x.reshape(-1, 1)), color='red', linewidth=2)
ax.set_title('Simple Linear Regression')
ax.set_xlabel('X Variable')
ax.set_ylabel('Y Variable')
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### 2. Multiple Regression: Adding Complexity

Now let's see how multiple variables interact:

```{python}
#| label: multiple-regression-python
#| fig-cap: Python multiple regression example
#| echo: true

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Set seed for reproducibility
np.random.seed(456)

# Create multiple regression example
n = 50
x1 = np.random.normal(10, 3, n)
x2 = np.random.normal(5, 2, n)
y = 2 * x1 + 0.5 * x2 + 3 + np.random.normal(0, 2, n)

# Fit multiple regression
X = np.column_stack([x1, x2])
model_multi = LinearRegression()
model_multi.fit(X, y)

# Display results
print(f"Coefficients: {model_multi.coef_}")
print(f"Intercept: {model_multi.intercept_:.3f}")
print(f"R-squared: {r2_score(y, model_multi.predict(X)):.3f}")

# Create pairs plot
data_df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
sns.pairplot(data_df)
plt.suptitle('Pairs Plot: Multiple Regression Variables', y=1.02)
plt.tight_layout()
plt.show()
```

::: {.callout-note}
## Statistical Significance at 5% Level

A coefficient is **statistically significant** when its p-value is less than 0.05.

- **p < 0.05**: Statistically significant
- **p ‚â• 0.05**: Not statistically significant

### Understanding Scientific Notation in P-values

Sometimes you'll see p-values written in scientific notation like `7.89e-4`. Don't panic! This is just a way to write very small numbers:

- **7.89e-4** means 7.89 √ó 10‚Åª‚Å¥ = 0.000789
- **2.34e-6** means 2.34 √ó 10‚Åª‚Å∂ = 0.00000234
- **1.23e-2** means 1.23 √ó 10‚Åª¬≤ = 0.0123

**The key rule:** If you see "e-" in a p-value, it's always a very small number (less than 1). The number after "e-" tells you how many zeros come before the first non-zero digit.

**Examples:**
- 7.89e-4 = 0.000789 (less than 0.05, so significant!)
- 2.34e-6 = 0.00000234 (way less than 0.05, so very significant!)
- 1.23e-2 = 0.0123 (less than 0.05, so significant!)

**Remember:** Statistical significance doesn't mean the effect is large or important - it just means we're confident the effect isn't zero.
:::

## The Problem of Non-Linearity: A Deeper Look

The "garbage can regression" problem occurs when we include variables in our regression models that create misleading results, even when they appear statistically significant. This happens in several ways:

1. **Random correlations:** Even random variables can appear correlated by chance
2. **Overfitting:** More variables can improve fit without improving understanding
3. **Multiple testing:** The more variables we test, the more likely we are to find spurious relationships
4. **Non-linear relationships:** Variables with U-shaped, exponential, or other non-linear relationships with the outcome are forced into a linear framework, creating misleading coefficients

### Why This Matters

In the real world, non-linear relationships can lead to:

- **False policy recommendations:** Basing decisions on spurious correlations or false causal relationships
- **Wasted resources:** Pursuing interventions that don't actually work
- **Loss of credibility:** When results can't be replicated or don't make sense
- **Ethical issues:** Making decisions that affect people's lives based on bad science

### The Solution

The key is to always ask:

1. **Does this make theoretical sense?** Is there a plausible mechanism?
2. **Is the relationship robust?** Does it hold across different samples and specifications?
3. **Are we overfitting?** Do we have enough data relative to the number of variables?
4. **Can we interpret the coefficients?** Do the results tell a coherent story?
5. **Is the relationship truly linear?** Check for non-linear patterns that linear regression can't capture
6. **Are we forcing the wrong functional form?** Consider if polynomial terms, interactions, or transformations are needed
7. **Split the sample into meaningful subsets:** Analyze different "statistical regimes" to see if relationships hold consistently across different parts of your data
8. **Use graphical diagnostics:** Don't rely blindly on "canned" regressions‚Äîvisualize the relationships to understand what's really happening

Remember: **Correlation is not causation, and regression coefficients can lie!** üìä
